# Task ID: P3.5.a - Design Comparison Procedure

**Sub-phase:** P3.5
**Scope:** Phase 3 - Orchestrator & Pipelines only
**Task type:** design

## Purpose

Design the procedure for comparing results from both simulation paths to validate the new orchestrator implementation.

## Context to load before running this task

- `.claude/v5-final-docs/phase-3-orchestrator/P3.5_run_comparison_and_rollback/P3.5.shared_context_run_comparison_expectations.md`
- `tests/run_comparison.py` (if exists)

## Outputs

### 1. Comparison Script Design

```python
#!/usr/bin/env python
"""
Phase 3 Comparison Script

Compares simulation results from legacy SDNController path and
new SDNOrchestrator path to verify equivalence.

Usage:
    python tests/run_comparison_p3.py [options]

Options:
    --config PATH       Config file to use
    --seed INT          Random seed (default: 42)
    --requests INT      Number of requests (default: 10000)
    --iterations INT    Number of iterations (default: 3)
    --tolerance FLOAT   Blocking tolerance (default: 0.02)
    --verbose           Enable verbose output
"""

from __future__ import annotations

import argparse
import json
import logging
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any

logger = logging.getLogger(__name__)


@dataclass
class ComparisonResult:
    """Results from a path comparison."""
    legacy_blocking: float
    new_blocking: float
    difference: float
    passed: bool
    details: dict[str, Any]


def run_legacy_path(config: dict, seed: int) -> dict:
    """Run simulation with legacy SDNController path."""
    from fusion.core.simulation import SimulationEngine

    config = config.copy()
    config['use_orchestrator'] = False

    logger.info("Running legacy path...")
    engine = SimulationEngine(config)
    engine.run(seed=seed)

    return {
        'blocking_probability': engine.stats.block_mean,
        'blocked_requests': engine.stats.blocked_requests,
        'total_requests': engine.stats.total_requests,
        'block_reasons': dict(engine.stats.stats_props.block_reasons_dict),
        'bit_rate_blocking': getattr(engine.stats, 'bit_rate_block_mean', None),
    }


def run_new_path(config: dict, seed: int) -> dict:
    """Run simulation with new SDNOrchestrator path."""
    from fusion.core.simulation import SimulationEngine

    config = config.copy()
    config['use_orchestrator'] = True

    logger.info("Running new path...")
    engine = SimulationEngine(config)
    engine.run(seed=seed)

    return {
        'blocking_probability': engine.stats.block_mean,
        'blocked_requests': engine.stats.blocked_requests,
        'total_requests': engine.stats.total_requests,
        'block_reasons': dict(engine.stats.stats_props.block_reasons_dict),
        'bit_rate_blocking': getattr(engine.stats, 'bit_rate_block_mean', None),
    }


def compare_results(
    legacy: dict,
    new: dict,
    tolerance: float = 0.02,
) -> ComparisonResult:
    """Compare results from both paths."""
    legacy_bp = legacy['blocking_probability'] or 0.0
    new_bp = new['blocking_probability'] or 0.0
    difference = abs(legacy_bp - new_bp)

    passed = difference < tolerance

    details = {
        'legacy': legacy,
        'new': new,
        'primary_metrics': {
            'blocking_difference': difference,
            'within_tolerance': passed,
        },
        'secondary_metrics': {
            'block_reasons_legacy': legacy['block_reasons'],
            'block_reasons_new': new['block_reasons'],
        },
    }

    return ComparisonResult(
        legacy_blocking=legacy_bp,
        new_blocking=new_bp,
        difference=difference,
        passed=passed,
        details=details,
    )


def print_report(result: ComparisonResult) -> None:
    """Print comparison report."""
    print("\n" + "=" * 60)
    print("PHASE 3 COMPARISON REPORT")
    print("=" * 60)

    print(f"\nLegacy blocking probability: {result.legacy_blocking:.4f}")
    print(f"New blocking probability:    {result.new_blocking:.4f}")
    print(f"Absolute difference:         {result.difference:.4f}")

    print("\n" + "-" * 60)
    if result.passed:
        print("RESULT: PASS - Within tolerance")
    else:
        print("RESULT: FAIL - Exceeds tolerance")
    print("-" * 60)

    # Secondary metrics
    print("\nBlock Reasons Comparison:")
    legacy_reasons = result.details['secondary_metrics']['block_reasons_legacy']
    new_reasons = result.details['secondary_metrics']['block_reasons_new']

    for reason in set(legacy_reasons.keys()) | set(new_reasons.keys()):
        legacy_count = legacy_reasons.get(reason, 0) or 0
        new_count = new_reasons.get(reason, 0) or 0
        print(f"  {reason}: legacy={legacy_count}, new={new_count}")


def main() -> int:
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Phase 3 comparison script")
    parser.add_argument('--config', type=Path, help="Config file")
    parser.add_argument('--seed', type=int, default=42, help="Random seed")
    parser.add_argument('--requests', type=int, default=10000)
    parser.add_argument('--iterations', type=int, default=3)
    parser.add_argument('--tolerance', type=float, default=0.02)
    parser.add_argument('--verbose', action='store_true')

    args = parser.parse_args()

    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    else:
        logging.basicConfig(level=logging.INFO)

    # Build config
    config = {
        'topology': 'NSFNet',
        'num_requests': args.requests,
        'max_iterations': args.iterations,
    }

    if args.config:
        config.update(load_config(args.config))

    # Run comparison
    legacy_results = run_legacy_path(config, args.seed)
    new_results = run_new_path(config, args.seed)

    # Compare
    result = compare_results(legacy_results, new_results, args.tolerance)

    # Report
    print_report(result)

    return 0 if result.passed else 1


if __name__ == '__main__':
    sys.exit(main())
```

### 2. Integration with Existing run_comparison.py

If `tests/run_comparison.py` already exists, add orchestrator support:

```python
# In existing run_comparison.py

def run_comparison_test(
    config_name: str,
    use_orchestrator: bool = False,
) -> dict:
    """
    Run comparison test with specified path.

    Args:
        config_name: Configuration to test
        use_orchestrator: Use new orchestrator path

    Returns:
        Test results dictionary
    """
    engine_props = load_config(config_name)
    engine_props['use_orchestrator'] = use_orchestrator

    # Run simulation
    results = run_simulation(engine_props)

    # Compare against baseline
    baseline = load_baseline(config_name)
    compare_results(results, baseline, abs_tol=0.02)

    return results


def run_dual_path_comparison(config_name: str) -> bool:
    """
    Run both paths and compare to each other.

    Returns:
        True if both paths produce equivalent results
    """
    legacy = run_comparison_test(config_name, use_orchestrator=False)
    new = run_comparison_test(config_name, use_orchestrator=True)

    bp_diff = abs(legacy['blocking_probability'] - new['blocking_probability'])

    if bp_diff < 0.02:
        print(f"PASS: {config_name} - difference {bp_diff:.4f}")
        return True
    else:
        print(f"FAIL: {config_name} - difference {bp_diff:.4f}")
        return False
```

### 3. CI/CD Integration

```yaml
# .github/workflows/phase3-comparison.yml
name: Phase 3 Comparison

on:
  pull_request:
    paths:
      - 'fusion/core/orchestrator.py'
      - 'fusion/core/pipeline_factory.py'
      - 'fusion/core/simulation.py'

jobs:
  comparison:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -e .[dev]

      - name: Run comparison
        run: |
          python tests/run_comparison_p3.py \
            --seed 42 \
            --requests 5000 \
            --iterations 2 \
            --tolerance 0.02
```

### 4. Test Procedure

```markdown
## Comparison Test Procedure

### Pre-requisites
1. Phase 3 implementation complete
2. All unit tests passing
3. Environment configured

### Steps

1. **Quick Validation** (< 5 min)
   ```bash
   python tests/run_comparison_p3.py --requests 1000 --iterations 1
   ```

2. **Standard Validation** (~ 15 min)
   ```bash
   python tests/run_comparison_p3.py --requests 10000 --iterations 3
   ```

3. **Full Validation** (~ 1 hour)
   ```bash
   python tests/run_comparison_p3.py --requests 100000 --iterations 5
   ```

### Expected Output

```
============================================================
PHASE 3 COMPARISON REPORT
============================================================

Legacy blocking probability: 0.0523
New blocking probability:    0.0531
Absolute difference:         0.0008

------------------------------------------------------------
RESULT: PASS - Within tolerance
------------------------------------------------------------
```
```

## Verification

After design, verify:
- [ ] Script structure defined
- [ ] Tolerance criteria clear
- [ ] Integration path documented
- [ ] CI/CD integration designed

## Next Task

After completing this design, proceed to `P3.5.b_design_rollback_plan.md`.
