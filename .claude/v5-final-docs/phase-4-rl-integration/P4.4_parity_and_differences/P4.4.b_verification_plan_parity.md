# Task ID: P4.4.b - Verification Plan: Parity Test Implementation

**Sub-phase:** P4.4
**Scope:** Phase 4 - RL Integration only
**Task type:** verification-plan

## Purpose

Implement the parity test suite and comparison scripts that validate `UnifiedSimEnv` against `GeneralSimEnv`.

## Context to load before running this task

- `.claude/v5-final-docs/phase-4-rl-integration/P4.4_parity_and_differences/P4.4.a_design_parity_tests.md`
- `.claude/v5-final-docs/phase-4-rl-integration/P4.4_parity_and_differences/P4.4.shared_context_parity_metrics.md`

## Outputs

### 1. Test File: test_rl_parity.py

Create `fusion/tests/rl/test_rl_parity.py`:

```python
"""Parity tests between GeneralSimEnv and UnifiedSimEnv.

These tests verify that UnifiedSimEnv produces results comparable
to GeneralSimEnv, within documented tolerances.
"""

import pytest
import numpy as np
import warnings
from typing import Tuple

# Import both environments
from fusion.modules.rl.gymnasium_envs import create_sim_env, EnvType


# Fixtures
@pytest.fixture
def legacy_env(env_config):
    """Create legacy GeneralSimEnv."""
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", DeprecationWarning)
        env = create_sim_env(env_config, env_type=EnvType.LEGACY)
        yield env
        env.close()


@pytest.fixture
def unified_env(env_config):
    """Create UnifiedSimEnv."""
    env = create_sim_env(env_config, env_type=EnvType.UNIFIED, wrap_action_mask=False)
    yield env
    env.close()


@pytest.fixture
def env_pair(env_config) -> Tuple:
    """Create both environments for comparison."""
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", DeprecationWarning)
        legacy = create_sim_env(env_config, env_type=EnvType.LEGACY)
        unified = create_sim_env(env_config, env_type=EnvType.UNIFIED, wrap_action_mask=False)
        yield legacy, unified
        legacy.close()
        unified.close()


# Configuration
PARITY_CONFIG = {
    "episodes": 10,
    "blocking_tolerance": 0.05,  # 5%
    "reward_tolerance": 0.10,    # 10%
    "agreement_threshold": 0.85, # 85% step agreement
    "num_steps": 100,
}


class TestDeterminism:
    """Level 1: Verify request sequences match with same seed."""

    def test_same_seed_same_source(self, env_pair):
        """Source node should match with same seed."""
        legacy, unified = env_pair
        legacy_obs, _ = legacy.reset(seed=42)
        unified_obs, _ = unified.reset(seed=42)

        legacy_src = np.argmax(legacy_obs["source"])
        unified_src = np.argmax(unified_obs["source"])

        assert legacy_src == unified_src, \
            f"Source mismatch: legacy={legacy_src}, unified={unified_src}"

    def test_same_seed_same_destination(self, env_pair):
        """Destination node should match with same seed."""
        legacy, unified = env_pair
        legacy_obs, _ = legacy.reset(seed=42)
        unified_obs, _ = unified.reset(seed=42)

        legacy_dst = np.argmax(legacy_obs["destination"])
        unified_dst = np.argmax(unified_obs["destination"])

        assert legacy_dst == unified_dst, \
            f"Destination mismatch: legacy={legacy_dst}, unified={unified_dst}"

    def test_request_sequence_determinism(self, env_pair):
        """Request sequence should be deterministic."""
        legacy, unified = env_pair
        legacy.reset(seed=123)
        unified.reset(seed=123)

        mismatches = 0
        for step in range(20):
            legacy_obs, _, term_l, _, _ = legacy.step(0)
            unified_obs, _, term_u, _, _ = unified.step(0)

            if term_l or term_u:
                break

            legacy_src = np.argmax(legacy_obs["source"])
            unified_src = np.argmax(unified_obs["source"])

            if legacy_src != unified_src:
                mismatches += 1

        # Allow at most 1 mismatch (due to different blocking)
        assert mismatches <= 1, f"Too many request mismatches: {mismatches}"


class TestObservationCompatibility:
    """Level 2: Verify observation structures are compatible."""

    def test_source_shape_matches(self, env_pair):
        """Source observation shape should match."""
        legacy, unified = env_pair
        legacy_obs, _ = legacy.reset(seed=1)
        unified_obs, _ = unified.reset(seed=1)

        assert legacy_obs["source"].shape == unified_obs["source"].shape

    def test_destination_shape_matches(self, env_pair):
        """Destination observation shape should match."""
        legacy, unified = env_pair
        legacy_obs, _ = legacy.reset(seed=1)
        unified_obs, _ = unified.reset(seed=1)

        assert legacy_obs["destination"].shape == unified_obs["destination"].shape

    def test_path_features_shape_matches(self, env_pair):
        """Path feature shapes should match."""
        legacy, unified = env_pair
        legacy_obs, _ = legacy.reset(seed=1)
        unified_obs, _ = unified.reset(seed=1)

        assert legacy_obs["slots_needed"].shape == unified_obs["slots_needed"].shape
        assert legacy_obs["path_lengths"].shape == unified_obs["path_lengths"].shape

    def test_unified_has_additional_features(self, unified_env):
        """Unified env should have is_feasible feature."""
        obs, _ = unified_env.reset(seed=1)
        assert "is_feasible" in obs, "is_feasible feature missing"

    def test_unified_has_action_mask(self, unified_env):
        """Unified env should provide action mask."""
        _, info = unified_env.reset(seed=1)
        assert "action_mask" in info, "action_mask missing from info"


class TestSingleStepBehavior:
    """Level 3: Verify individual steps produce similar outcomes."""

    def test_reward_sign_agreement(self, env_pair):
        """Reward signs should agree most of the time."""
        legacy, unified = env_pair
        legacy.reset(seed=42)
        unified.reset(seed=42)

        agreements = 0
        total = 0

        for _ in range(PARITY_CONFIG["num_steps"]):
            action = 0

            _, r_legacy, term_l, _, _ = legacy.step(action)
            _, r_unified, term_u, _, _ = unified.step(action)

            total += 1
            if (r_legacy > 0) == (r_unified > 0):
                agreements += 1

            if term_l or term_u:
                break

        agreement_rate = agreements / total
        threshold = PARITY_CONFIG["agreement_threshold"]

        assert agreement_rate >= threshold, \
            f"Agreement rate {agreement_rate:.2%} below threshold {threshold:.2%}"

    @pytest.mark.xfail(reason="Exact match may fail due to feasibility differences")
    def test_exact_reward_match(self, env_pair):
        """Rewards should match exactly (may fail for known reasons)."""
        legacy, unified = env_pair
        legacy.reset(seed=42)
        unified.reset(seed=42)

        for step in range(10):
            _, r_legacy, _, _, _ = legacy.step(0)
            _, r_unified, _, _, _ = unified.step(0)

            assert r_legacy == r_unified, f"Step {step}: {r_legacy} != {r_unified}"


class TestEpisodeMetrics:
    """Level 4: Verify episode-level statistics are comparable."""

    def test_blocking_probability_within_tolerance(self, env_pair):
        """Blocking probability should be within tolerance."""
        legacy, unified = env_pair

        legacy_bp = self._compute_blocking_prob(legacy)
        unified_bp = self._compute_blocking_prob(unified)

        diff = abs(unified_bp - legacy_bp)
        tolerance = PARITY_CONFIG["blocking_tolerance"]

        assert diff <= tolerance, \
            f"Blocking diff {diff:.4f} exceeds tolerance {tolerance}"

    def test_mean_reward_within_tolerance(self, env_pair):
        """Mean episode reward should be within tolerance."""
        legacy, unified = env_pair

        legacy_mean = self._compute_mean_reward(legacy)
        unified_mean = self._compute_mean_reward(unified)

        if legacy_mean != 0:
            diff_pct = abs(unified_mean - legacy_mean) / abs(legacy_mean)
        else:
            diff_pct = 0 if unified_mean == 0 else 1.0

        tolerance = PARITY_CONFIG["reward_tolerance"]

        assert diff_pct <= tolerance, \
            f"Reward diff {diff_pct:.2%} exceeds tolerance {tolerance:.2%}"

    def _compute_blocking_prob(self, env, episodes=None, seed_base=0):
        """Compute average blocking probability."""
        episodes = episodes or PARITY_CONFIG["episodes"]
        blocked = 0
        total = 0

        for ep in range(episodes):
            env.reset(seed=seed_base + ep)
            done = False
            while not done:
                _, reward, terminated, truncated, _ = env.step(0)
                done = terminated or truncated
                total += 1
                if reward < 0:
                    blocked += 1

        return blocked / total if total > 0 else 0

    def _compute_mean_reward(self, env, episodes=None, seed_base=0):
        """Compute mean episode reward."""
        episodes = episodes or PARITY_CONFIG["episodes"]
        rewards = []

        for ep in range(episodes):
            env.reset(seed=seed_base + ep)
            total = 0
            done = False
            while not done:
                _, reward, terminated, truncated, _ = env.step(0)
                done = terminated or truncated
                total += reward
            rewards.append(total)

        return np.mean(rewards)


class TestParityReport:
    """Generate parity report for documentation."""

    def test_generate_parity_report(self, env_pair, tmp_path):
        """Generate and save parity report."""
        legacy, unified = env_pair

        report = self._generate_report(legacy, unified)

        # Save report
        report_path = tmp_path / "parity_report.md"
        report_path.write_text(report)

        # Verify report contains key sections
        assert "Blocking Probability" in report
        assert "Episode Rewards" in report
        assert "Status:" in report

    def _generate_report(self, legacy, unified):
        """Generate markdown parity report."""
        legacy_bp = self._compute_blocking_prob(legacy)
        unified_bp = self._compute_blocking_prob(unified)
        bp_diff = abs(unified_bp - legacy_bp)
        bp_status = "PASS" if bp_diff <= 0.05 else "FAIL"

        legacy_reward = self._compute_mean_reward(legacy)
        unified_reward = self._compute_mean_reward(unified)

        return f"""# RL Environment Parity Report

## Blocking Probability
- Legacy: {legacy_bp:.4f}
- Unified: {unified_bp:.4f}
- Difference: {bp_diff:.4f}
- Status: {bp_status}

## Episode Rewards
- Legacy Mean: {legacy_reward:.2f}
- Unified Mean: {unified_reward:.2f}

## Summary
All tests: {bp_status}
"""

    def _compute_blocking_prob(self, env, episodes=5):
        blocked = total = 0
        for ep in range(episodes):
            env.reset(seed=ep)
            done = False
            while not done:
                _, r, term, trunc, _ = env.step(0)
                done = term or trunc
                total += 1
                if r < 0:
                    blocked += 1
        return blocked / total if total else 0

    def _compute_mean_reward(self, env, episodes=5):
        rewards = []
        for ep in range(episodes):
            env.reset(seed=ep)
            total = 0
            done = False
            while not done:
                _, r, term, trunc, _ = env.step(0)
                done = term or trunc
                total += r
            rewards.append(total)
        return np.mean(rewards)
```

### 2. Comparison Script

Create `scripts/compare_rl_envs.py`:

```python
#!/usr/bin/env python
"""Compare GeneralSimEnv and UnifiedSimEnv for parity validation."""

import argparse
import json
import sys
import warnings
from pathlib import Path

import numpy as np

from fusion.modules.rl.gymnasium_envs import create_sim_env, EnvType


def parse_args():
    parser = argparse.ArgumentParser(description="Compare RL environments")
    parser.add_argument("--config", type=str, help="Config file path")
    parser.add_argument("--episodes", type=int, default=10, help="Number of episodes")
    parser.add_argument("--seed-base", type=int, default=0, help="Base seed")
    parser.add_argument("--output", type=str, help="Output JSON path")
    parser.add_argument("--verbose", action="store_true", help="Verbose output")
    return parser.parse_args()


def compute_metrics(env, episodes, seed_base, verbose=False):
    """Compute metrics for an environment."""
    blocking_rates = []
    episode_rewards = []

    for ep in range(episodes):
        seed = seed_base + ep
        env.reset(seed=seed)

        blocked = 0
        total = 0
        reward_sum = 0
        done = False

        while not done:
            action = 0  # Heuristic: always first path
            _, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            total += 1
            reward_sum += reward
            if reward < 0:
                blocked += 1

        blocking_rate = blocked / total if total > 0 else 0
        blocking_rates.append(blocking_rate)
        episode_rewards.append(reward_sum)

        if verbose:
            print(f"  Episode {ep}: BP={blocking_rate:.4f}, Reward={reward_sum:.2f}")

    return {
        "blocking_mean": np.mean(blocking_rates),
        "blocking_std": np.std(blocking_rates),
        "reward_mean": np.mean(episode_rewards),
        "reward_std": np.std(episode_rewards),
    }


def compare_envs(config, episodes, seed_base, verbose=False):
    """Compare legacy and unified environments."""
    print("Creating environments...")

    with warnings.catch_warnings():
        warnings.simplefilter("ignore", DeprecationWarning)
        legacy_env = create_sim_env(config, env_type=EnvType.LEGACY)

    unified_env = create_sim_env(config, env_type=EnvType.UNIFIED, wrap_action_mask=False)

    print("\nRunning legacy environment...")
    legacy_metrics = compute_metrics(legacy_env, episodes, seed_base, verbose)

    print("\nRunning unified environment...")
    unified_metrics = compute_metrics(unified_env, episodes, seed_base, verbose)

    legacy_env.close()
    unified_env.close()

    # Compute differences
    bp_diff = abs(unified_metrics["blocking_mean"] - legacy_metrics["blocking_mean"])
    reward_diff = abs(unified_metrics["reward_mean"] - legacy_metrics["reward_mean"])
    if legacy_metrics["reward_mean"] != 0:
        reward_diff_pct = reward_diff / abs(legacy_metrics["reward_mean"])
    else:
        reward_diff_pct = 0 if unified_metrics["reward_mean"] == 0 else 1.0

    results = {
        "legacy": legacy_metrics,
        "unified": unified_metrics,
        "comparison": {
            "blocking_diff": bp_diff,
            "blocking_pass": bp_diff <= 0.05,
            "reward_diff_pct": reward_diff_pct,
            "reward_pass": reward_diff_pct <= 0.10,
        },
        "overall_pass": bp_diff <= 0.05 and reward_diff_pct <= 0.10,
    }

    return results


def print_report(results):
    """Print comparison report."""
    print("\n" + "=" * 60)
    print("PARITY COMPARISON REPORT")
    print("=" * 60)

    print("\nBlocking Probability:")
    print(f"  Legacy:  {results['legacy']['blocking_mean']:.4f} (+/- {results['legacy']['blocking_std']:.4f})")
    print(f"  Unified: {results['unified']['blocking_mean']:.4f} (+/- {results['unified']['blocking_std']:.4f})")
    print(f"  Diff:    {results['comparison']['blocking_diff']:.4f}")
    print(f"  Status:  {'PASS' if results['comparison']['blocking_pass'] else 'FAIL'}")

    print("\nEpisode Rewards:")
    print(f"  Legacy:  {results['legacy']['reward_mean']:.2f} (+/- {results['legacy']['reward_std']:.2f})")
    print(f"  Unified: {results['unified']['reward_mean']:.2f} (+/- {results['unified']['reward_std']:.2f})")
    print(f"  Diff:    {results['comparison']['reward_diff_pct']:.2%}")
    print(f"  Status:  {'PASS' if results['comparison']['reward_pass'] else 'FAIL'}")

    print("\n" + "=" * 60)
    print(f"OVERALL: {'PASS' if results['overall_pass'] else 'FAIL'}")
    print("=" * 60)


def main():
    args = parse_args()

    # Load config (simplified - in practice would load from file)
    config = {}  # TODO: Load from args.config

    results = compare_envs(config, args.episodes, args.seed_base, args.verbose)
    print_report(results)

    if args.output:
        Path(args.output).write_text(json.dumps(results, indent=2))
        print(f"\nResults saved to {args.output}")

    return 0 if results["overall_pass"] else 1


if __name__ == "__main__":
    sys.exit(main())
```

### 3. Test Commands

```bash
# Run all parity tests
pytest fusion/tests/rl/test_rl_parity.py -v

# Run with coverage
pytest fusion/tests/rl/test_rl_parity.py --cov=fusion/rl --cov-report=term-missing

# Run comparison script
python scripts/compare_rl_envs.py --episodes 20 --verbose

# Generate report
python scripts/compare_rl_envs.py --episodes 10 --output parity_report.json
```

### 4. CI Integration

Add to CI workflow:

```yaml
parity-tests:
  runs-on: ubuntu-latest
  steps:
    - uses: actions/checkout@v3
    - name: Run parity tests
      run: |
        pytest fusion/tests/rl/test_rl_parity.py -v --tb=short
    - name: Run comparison script
      run: |
        python scripts/compare_rl_envs.py --episodes 10 --output parity_report.json
    - name: Upload report
      uses: actions/upload-artifact@v3
      with:
        name: parity-report
        path: parity_report.json
```

## Verification Checklist

- [ ] test_rl_parity.py created and runnable
- [ ] All 5 test levels implemented
- [ ] Comparison script created
- [ ] Tolerances match design spec
- [ ] Expected failures marked with xfail
- [ ] CI integration defined

## Next Task

Proceed to **P4.4.c** to create differences documentation.
