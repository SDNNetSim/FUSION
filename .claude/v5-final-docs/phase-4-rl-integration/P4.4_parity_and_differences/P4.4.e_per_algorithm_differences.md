# Task ID: P4.4.e - Per-Algorithm Differences Matrix

**Sub-phase:** P4.4
**Scope:** Phase 4 - RL Integration only
**Task type:** documentation
**Priority:** High (users need to understand impact on their models)

## Purpose

Document the specific differences between `GeneralSimEnv` and `UnifiedSimEnv` for each supported RL algorithm, including impact on trained models, expected behavioral changes, and mitigation strategies.

---

## Executive Summary

| Algorithm | Model Compatible | Behavior Change | Performance Impact | Recommended Action |
|-----------|-----------------|-----------------|-------------------|-------------------|
| PPO | No | Moderate | +5-15% | Retrain with MaskablePPO |
| A2C | No | Moderate | +5-10% | Retrain with masking |
| DQN | No | Moderate | Variable | Retrain with masking |
| QR-DQN | No | Moderate | Variable | Retrain with masking |
| Q-Learning | No | High | Variable | Retrain with discretizer |
| Bandits | No | Low | Minimal | Retrain arm estimates |
| BC | Yes* | Low | Minimal | Use adapter |
| IQL | Yes* | Low | Minimal | Use adapter |

*With OfflinePolicyAdapter

---

## Detailed Differences by Algorithm

### PPO (Proximal Policy Optimization)

#### Observation Space Changes

| Feature | Legacy | Unified | Impact |
|---------|--------|---------|--------|
| `paths_cong` | Present | Renamed `congestion` | Key rename breaks model |
| `is_feasible` | Absent | Present | New feature (additive) |
| Shape | Identical | Identical | No shape change |

#### Feasibility Computation

**Legacy:**
- Mock feasibility via `mock_handle_arrival()`
- May differ from actual allocation outcome
- Higher false positive rate

**Unified:**
- Real feasibility via `spectrum_pipeline.find_spectrum(allocate=False)`
- Matches actual allocation behavior
- Accurate feasibility

**Impact:** Policies trained on legacy may learn to rely on inaccurate feasibility signals.

#### Action Masking

**Legacy:**
- No action masking in info dict
- Agent learns through reward signal (trial and error)
- Slower convergence

**Unified:**
- Action mask in `info["action_mask"]`
- With MaskablePPO: impossible actions blocked
- Faster convergence, fewer blocking events

**Expected Changes:**
- 5-15% improvement in blocking probability
- Faster training convergence
- Different exploration patterns

#### Rollout Differences

**Legacy:**
```
Step 1: Agent selects action 2 (infeasible)
        -> Allocation fails
        -> Reward: -1.0
        -> Agent learns "action 2 is bad"
```

**Unified (with masking):**
```
Step 1: Mask = [True, False, True]
        -> Agent cannot select action 1
        -> Agent selects action 0 or 2
        -> Higher success rate
```

#### Recommended Migration

```python
# Legacy
from stable_baselines3 import PPO
model = PPO("MultiInputPolicy", legacy_env)

# Unified
from sb3_contrib import MaskablePPO
env = UnifiedSimEnv(config)
env = ActionMaskWrapper(env)
model = MaskablePPO("MultiInputPolicy", env)
model.learn(total_timesteps=100000)  # Retrain from scratch
```

---

### A2C (Advantage Actor-Critic)

#### Differences

Same as PPO for:
- Observation space changes
- Feasibility computation
- Action masking benefits

#### A2C-Specific Notes

- A2C is more sensitive to hyperparameters than PPO
- Action masking has larger relative impact
- May need learning rate adjustment after migration

#### Recommended Migration

```python
# Use standard A2C with action masking callback
from stable_baselines3 import A2C
from fusion.rl.environments import UnifiedSimEnv, ActionMaskWrapper

env = UnifiedSimEnv(config)
env = ActionMaskWrapper(env)

# A2C doesn't have native masking, use custom policy or accept exploration waste
model = A2C("MultiInputPolicy", env, learning_rate=7e-4)
```

---

### DQN (Deep Q-Network)

#### Key Differences

| Aspect | Legacy | Unified |
|--------|--------|---------|
| Q-value interpretation | All actions | Masked actions = -inf |
| Exploration (epsilon) | All actions | Only valid actions |
| Target network | Standard | May need masking |

#### DQN-Specific Considerations

**Exploration Strategy:**
- Legacy: Epsilon-greedy over all k_paths
- Unified: Epsilon-greedy over valid paths only

**Q-Value Learning:**
- Legacy: Q(s,a) learned for all actions
- Unified: Q(s,a) for masked actions should be ignored

**Double DQN Impact:**
- Max Q-value selection must respect mask
- Target computation affected

#### Implementation Notes

```python
class MaskedDQN:
    """DQN with action masking support."""

    def predict(self, obs, action_mask):
        q_values = self.q_net(obs)

        # Mask invalid actions
        q_values[~action_mask] = float("-inf")

        if self.training and np.random.random() < self.epsilon:
            # Explore only valid actions
            valid_actions = np.where(action_mask)[0]
            return np.random.choice(valid_actions)

        return q_values.argmax()

    def compute_target(self, next_obs, next_mask, reward, done):
        with torch.no_grad():
            next_q = self.target_net(next_obs)
            next_q[~next_mask] = float("-inf")
            max_next_q = next_q.max()

        return reward + (1 - done) * self.gamma * max_next_q
```

---

### QR-DQN (Quantile Regression DQN)

#### Differences from Standard DQN

- Distributional RL: learns Q-value distribution
- Quantile targets affected by masking
- Risk-sensitive behavior may change

#### QR-DQN-Specific Notes

**Quantile Computation:**
- Must mask invalid actions before quantile aggregation
- Risk measures (CVaR, etc.) only over valid actions

**Expected Impact:**
- Similar to DQN for basic metrics
- Risk-aware policies may behave differently
- Uncertainty estimates more reliable (no invalid action noise)

---

### Q-Learning (Tabular)

#### Fundamental Changes

| Aspect | Legacy | Unified |
|--------|--------|---------|
| State space | Discrete (pre-defined) | Continuous (Dict) |
| State representation | Index | Feature vector |
| Q-table size | Fixed | Requires discretization |

#### State Discretization Impact

**Legacy state index:**
- Pre-computed based on request parameters
- Fixed number of states

**Unified observation:**
- Continuous features
- Requires discretization or function approximation

**Example State Mapping:**

```python
# Legacy: state = hash(src, dst, bandwidth, time_bin)
state_index = 1234

# Unified: state = {source: [0,0,1,...], destination: [...], ...}
# Needs discretization
discretizer = StateDiscretizer(obs_space, num_bins=10)
state_index = discretizer.discretize(obs)  # May be much larger
```

#### Q-Table Compatibility

**Not compatible.** Different state representations require complete retraining.

#### Migration Considerations

1. **State space explosion:** Discretizing Dict space creates many more states
2. **Generalization:** Consider switching to function approximation (DQN)
3. **Action masking:** Q-values for masked actions should not be updated

---

### Bandits (Epsilon-Greedy, UCB)

#### Differences

| Aspect | Legacy | Unified |
|--------|--------|---------|
| Arm availability | All arms always | Masked arms unavailable |
| Statistics | Global per arm | Per-arm, respecting masks |
| Exploration | Full k_paths | Valid paths only |

#### Arm Statistics

**Legacy:**
- Each arm accumulates statistics globally
- All arms pulled over time

**Unified:**
- Masked arms not pulled
- Statistics only for valid arms
- May have uneven sample counts

#### UCB-Specific Notes

**Upper Confidence Bound calculation:**
```python
# Legacy
ucb = mean[arm] + c * sqrt(log(total_pulls) / arm_pulls[arm])

# Unified: only consider valid arms
for arm in range(k_paths):
    if action_mask[arm]:
        ucb[arm] = mean[arm] + c * sqrt(log(valid_total) / arm_pulls[arm])
    else:
        ucb[arm] = -inf  # Never select
```

**Impact:**
- UCB bounds may be tighter (more focused exploration)
- Faster convergence to good arms
- Arm pull counts more meaningful

---

### BC (Behavior Cloning)

#### State Format Differences

**Legacy state format (expected by model):**
```python
{
    "src": 0,
    "dst": 5,
    "slots_needed": 4,
    "est_remaining_time": 0.5,
    "is_disaster": 0,
    "paths": [
        {"path_hops": 2, "min_residual_slots": 0.7, ...},
        ...
    ]
}
```

**Unified observation format:**
```python
{
    "source": np.array([1, 0, 0, ...]),  # One-hot
    "destination": np.array([0, 0, 0, 0, 0, 1, ...]),
    "holding_time": np.array([0.5]),
    "slots_needed": np.array([4, 6, -1]),
    ...
}
```

#### Adapter Usage

The `OfflinePolicyAdapter` converts between formats:

```python
# Unified observation -> Legacy state
adapter = OfflinePolicyAdapter(bc_policy, rl_adapter, config)

# Internal conversion:
# 1. Extract src, dst from one-hot
# 2. Build path features from PathOption objects
# 3. Add disaster state if available
# 4. Return legacy-format state dict
```

#### Compatibility Notes

- **Pre-trained BC models work** via adapter
- No retraining needed
- Minor performance difference possible (state conversion)

---

### IQL (Implicit Q-Learning)

#### Differences from BC

- Same state format requirements
- Same adapter compatibility
- IQL is more conservative (stays close to behavior policy)

#### IQL-Specific Notes

- Conservative policy beneficial when feasibility differs
- Out-of-distribution handling helps with new feasibility patterns
- Expected to be more robust to environment changes

---

## Parity Testing Protocol

### Metric Comparison

For each algorithm, compare:

| Metric | Tolerance | Notes |
|--------|-----------|-------|
| Blocking probability | +/- 5% | May improve with masking |
| Average reward | +/- 10% | Higher with masking |
| Training time | +/- 20% | Faster with masking |
| Convergence episodes | +/- 30% | Fewer with masking |

### Test Procedure

```python
def compare_environments(config, algorithm, seeds=[42, 123, 456]):
    """Run parity comparison for algorithm."""
    legacy_results = []
    unified_results = []

    for seed in seeds:
        # Legacy
        legacy_env = SimEnv(sim_dict={"s1": config})
        legacy_model = train_algorithm(algorithm, legacy_env, seed)
        legacy_results.append(evaluate(legacy_model, legacy_env, seed))

        # Unified
        unified_env = UnifiedSimEnv(config)
        unified_env = ActionMaskWrapper(unified_env)
        unified_model = train_algorithm(algorithm, unified_env, seed)
        unified_results.append(evaluate(unified_model, unified_env, seed))

    # Compare
    return {
        "blocking_diff": np.mean([u["blocking"] - l["blocking"]
                                  for u, l in zip(unified_results, legacy_results)]),
        "reward_diff": np.mean([u["reward"] - l["reward"]
                                for u, l in zip(unified_results, legacy_results)]),
    }
```

---

## Summary Recommendations

### Quick Migration (Minimal Effort)

For users who need to migrate quickly:

1. **BC/IQL:** Use `OfflinePolicyAdapter` - no retraining
2. **PPO:** Switch to MaskablePPO, retrain
3. **Others:** Use factory with `env_type="legacy"` until ready

### Best Results (Recommended)

For users who want optimal performance:

1. **All online RL:** Retrain with action masking
2. **Tabular methods:** Switch to function approximation or retrain
3. **Offline RL:** Retrain with new dataset from unified env

### Gradual Migration

1. Run both environments in parallel
2. Compare metrics
3. Switch when confident
4. Keep legacy as rollback option

---

## Verification Checklist

- [ ] PPO differences documented and tested
- [ ] A2C differences documented and tested
- [ ] DQN differences documented and tested
- [ ] QR-DQN differences documented and tested
- [ ] Q-Learning differences documented and tested
- [ ] Bandit differences documented and tested
- [ ] BC compatibility verified with adapter
- [ ] IQL compatibility verified with adapter
- [ ] Parity metrics within tolerance
- [ ] Migration paths validated

## Next Task

Proceed to **P4.4.f: Graph Feature Extractor Parity**.
