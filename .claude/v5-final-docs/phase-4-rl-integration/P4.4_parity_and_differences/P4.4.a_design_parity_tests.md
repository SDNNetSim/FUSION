# Task ID: P4.4.a - Design Parity Test Strategy

**Sub-phase:** P4.4
**Scope:** Phase 4 - RL Integration only
**Task type:** design

## Purpose

Design the parity testing strategy to validate that `UnifiedSimEnv` produces results comparable to `GeneralSimEnv`, identifying intentional differences and catching regressions.

## Context to load before running this task

- `.claude/v5-final-docs/phase-4-rl-integration/P4.4_parity_and_differences/P4.4.shared_context_parity_metrics.md`
- `.claude/v5-final-docs/phase-4-rl-integration/P4.2_unified_env/P4.2.a_context_extraction_general_sim_env.md`
- `.claude/v4-docs/testing/phase_4_testing.md`

## Outputs

### 1. Test Hierarchy

```
Parity Tests
│
├── Level 1: Determinism
│   ├── Same seed produces same request sequence
│   └── Traffic generation is identical
│
├── Level 2: Observation Compatibility
│   ├── Observation keys overlap correctly
│   ├── Observation shapes match
│   └── Observation ranges are valid
│
├── Level 3: Single-Step Equivalence
│   ├── Same action produces same outcome type (success/block)
│   └── Same action produces same reward sign
│
├── Level 4: Episode Equivalence
│   ├── Similar blocking probability
│   └── Similar cumulative reward
│
└── Level 5: Statistical Equivalence
    ├── Multiple episode comparison
    └── Distribution similarity
```

### 2. Level 1: Determinism Tests

```python
class TestDeterminism:
    """Verify request sequences match with same seed."""

    def test_same_seed_same_first_request(
        self, legacy_env, unified_env
    ):
        """First request should match."""
        legacy_obs, _ = legacy_env.reset(seed=42)
        unified_obs, _ = unified_env.reset(seed=42)

        # Source and destination should match
        assert np.argmax(legacy_obs["source"]) == np.argmax(unified_obs["source"])
        assert np.argmax(legacy_obs["destination"]) == np.argmax(unified_obs["destination"])

    def test_request_sequence_matches(
        self, legacy_env, unified_env, num_requests=20
    ):
        """Request sequence should match across steps."""
        legacy_env.reset(seed=42)
        unified_env.reset(seed=42)

        for step in range(num_requests):
            legacy_obs, _, _, _, _ = legacy_env.step(0)
            unified_obs, _, _, _, _ = unified_env.step(0)

            legacy_src = np.argmax(legacy_obs["source"])
            unified_src = np.argmax(unified_obs["source"])

            legacy_dst = np.argmax(legacy_obs["destination"])
            unified_dst = np.argmax(unified_obs["destination"])

            assert legacy_src == unified_src, f"Source mismatch at step {step}"
            assert legacy_dst == unified_dst, f"Destination mismatch at step {step}"
```

### 3. Level 2: Observation Compatibility Tests

```python
class TestObservationCompatibility:
    """Verify observation structures are compatible."""

    LEGACY_KEYS = {"source", "destination", "holding_time", "slots_needed",
                   "path_lengths", "paths_cong", "available_slots"}

    UNIFIED_KEYS = {"source", "destination", "holding_time", "slots_needed",
                    "path_lengths", "congestion", "available_slots", "is_feasible"}

    RENAMED_KEYS = {"paths_cong": "congestion"}

    def test_common_keys_present(self, legacy_env, unified_env):
        """Common keys should be present in both."""
        legacy_env.reset(seed=1)
        unified_env.reset(seed=1)

        legacy_keys = set(legacy_env.observation_space.spaces.keys())
        unified_keys = set(unified_env.observation_space.spaces.keys())

        # Account for renamed keys
        adjusted_legacy = {self.RENAMED_KEYS.get(k, k) for k in legacy_keys}
        common = adjusted_legacy & unified_keys

        assert len(common) >= 5, "Should have at least 5 common keys"

    def test_observation_shapes_match(self, legacy_env, unified_env):
        """Common observation shapes should match."""
        legacy_obs, _ = legacy_env.reset(seed=1)
        unified_obs, _ = unified_env.reset(seed=1)

        for legacy_key, unified_key in [
            ("source", "source"),
            ("destination", "destination"),
            ("slots_needed", "slots_needed"),
            ("path_lengths", "path_lengths"),
            ("paths_cong", "congestion"),
            ("available_slots", "available_slots"),
        ]:
            if legacy_key in legacy_obs and unified_key in unified_obs:
                assert legacy_obs[legacy_key].shape == unified_obs[unified_key].shape

    def test_unified_has_is_feasible(self, unified_env):
        """Unified env should have is_feasible feature."""
        obs, _ = unified_env.reset(seed=1)
        assert "is_feasible" in obs
```

### 4. Level 3: Single-Step Equivalence Tests

```python
class TestSingleStepEquivalence:
    """Verify individual steps produce similar outcomes."""

    def test_same_action_same_outcome_type(
        self, legacy_env, unified_env
    ):
        """Same action should produce same success/failure."""
        legacy_env.reset(seed=42)
        unified_env.reset(seed=42)

        for _ in range(10):
            action = 0  # First path

            _, r_legacy, _, _, _ = legacy_env.step(action)
            _, r_unified, _, _, _ = unified_env.step(action)

            # Both should succeed or both should fail
            legacy_success = r_legacy > 0
            unified_success = r_unified > 0

            # Note: May not always match due to feasibility differences
            # Track agreement rate instead of asserting equality

    def test_reward_agreement_rate(
        self, legacy_env, unified_env, num_steps=100, min_agreement=0.90
    ):
        """Reward signs should agree at least 90% of the time."""
        legacy_env.reset(seed=42)
        unified_env.reset(seed=42)

        agreements = 0

        for step in range(num_steps):
            action = 0

            _, r_legacy, term_l, _, _ = legacy_env.step(action)
            _, r_unified, term_u, _, _ = unified_env.step(action)

            if (r_legacy > 0) == (r_unified > 0):
                agreements += 1

            if term_l or term_u:
                break

        agreement_rate = agreements / min(step + 1, num_steps)
        assert agreement_rate >= min_agreement, \
            f"Agreement rate {agreement_rate:.2%} below threshold {min_agreement:.2%}"
```

### 5. Level 4: Episode Equivalence Tests

```python
class TestEpisodeEquivalence:
    """Verify episode-level statistics are comparable."""

    def test_blocking_probability_within_tolerance(
        self, legacy_env, unified_env, episodes=10, tolerance=0.05
    ):
        """Blocking probability should be within 5%."""
        def compute_blocking(env, episodes, seed_base):
            blocked = 0
            total = 0
            for ep in range(episodes):
                env.reset(seed=seed_base + ep)
                done = False
                while not done:
                    action = 0
                    _, reward, terminated, truncated, _ = env.step(action)
                    done = terminated or truncated
                    total += 1
                    if reward < 0:
                        blocked += 1
            return blocked / total if total > 0 else 0

        legacy_bp = compute_blocking(legacy_env, episodes, 0)
        unified_bp = compute_blocking(unified_env, episodes, 0)

        diff = abs(unified_bp - legacy_bp)
        assert diff <= tolerance, \
            f"Blocking probability diff {diff:.4f} exceeds tolerance {tolerance}"

    def test_cumulative_reward_comparable(
        self, legacy_env, unified_env, episodes=10, tolerance=0.10
    ):
        """Cumulative rewards should be within 10%."""
        def compute_mean_reward(env, episodes, seed_base):
            rewards = []
            for ep in range(episodes):
                env.reset(seed=seed_base + ep)
                total = 0
                done = False
                while not done:
                    action = 0
                    _, reward, terminated, truncated, _ = env.step(action)
                    done = terminated or truncated
                    total += reward
                rewards.append(total)
            return np.mean(rewards)

        legacy_mean = compute_mean_reward(legacy_env, episodes, 0)
        unified_mean = compute_mean_reward(unified_env, episodes, 0)

        if legacy_mean != 0:
            diff_pct = abs(unified_mean - legacy_mean) / abs(legacy_mean)
        else:
            diff_pct = 0 if unified_mean == 0 else 1.0

        assert diff_pct <= tolerance, \
            f"Reward diff {diff_pct:.2%} exceeds tolerance {tolerance:.2%}"
```

### 6. Level 5: Statistical Equivalence Tests

```python
class TestStatisticalEquivalence:
    """Verify statistical properties are similar."""

    def test_reward_distribution_similar(
        self, legacy_env, unified_env, episodes=20, p_threshold=0.05
    ):
        """Reward distributions should not be significantly different."""
        from scipy import stats

        legacy_rewards = self._collect_episode_rewards(legacy_env, episodes)
        unified_rewards = self._collect_episode_rewards(unified_env, episodes)

        # Kolmogorov-Smirnov test
        statistic, p_value = stats.ks_2samp(legacy_rewards, unified_rewards)

        # Fail if distributions are significantly different
        # (p < 0.05 means significantly different)
        assert p_value > p_threshold or statistic < 0.3, \
            f"Distributions significantly different (p={p_value:.4f}, stat={statistic:.4f})"

    def test_blocking_rate_stable(
        self, unified_env, episodes=20, std_threshold=0.05
    ):
        """Unified env blocking rate should be stable."""
        rates = []
        for ep in range(episodes):
            unified_env.reset(seed=ep * 100)
            blocked = 0
            total = 0
            done = False
            while not done:
                _, reward, terminated, truncated, _ = unified_env.step(0)
                done = terminated or truncated
                total += 1
                if reward < 0:
                    blocked += 1
            rates.append(blocked / total if total > 0 else 0)

        std = np.std(rates)
        assert std <= std_threshold, \
            f"Blocking rate std {std:.4f} exceeds threshold {std_threshold}"

    def _collect_episode_rewards(self, env, episodes):
        rewards = []
        for ep in range(episodes):
            env.reset(seed=ep)
            total = 0
            done = False
            while not done:
                _, reward, terminated, truncated, _ = env.step(0)
                done = terminated or truncated
                total += reward
            rewards.append(total)
        return rewards
```

### 7. Test Configuration

```python
# conftest.py additions

@pytest.fixture(params=[42, 123, 999])
def multi_seed(request):
    """Parameterize tests with multiple seeds."""
    return request.param


@pytest.fixture
def parity_config():
    """Configuration for parity testing."""
    return {
        "episodes": 10,
        "blocking_tolerance": 0.05,
        "reward_tolerance": 0.10,
        "agreement_threshold": 0.90,
        "statistical_p_threshold": 0.05,
    }
```

### 8. Expected Failures and Skip Conditions

```python
# Mark tests that may fail due to intentional differences

@pytest.mark.xfail(
    reason="Feasibility may differ due to real vs mock spectrum check"
)
def test_exact_step_match():
    """This test may fail and that's expected."""
    pass


@pytest.mark.skipif(
    not LEGACY_ENV_AVAILABLE,
    reason="Legacy environment not available"
)
def test_requires_legacy():
    """Skip if legacy env not present."""
    pass
```

## Verification

- [ ] All 5 test levels defined
- [ ] Tolerances documented and justified
- [ ] Expected failures marked
- [ ] Skip conditions defined
- [ ] Test fixtures complete

## Next Task

Proceed to **P4.4.b** to implement the parity test suite.
