# Task ID: P4.3.e - Algorithm-Specific Migration Guide

**Sub-phase:** P4.3
**Scope:** Phase 4 - RL Integration only
**Task type:** migration-guide
**Priority:** High (users need guidance for each algorithm)

## Purpose

Provide detailed migration instructions for each RL algorithm supported by FUSION, documenting what changes are needed to migrate from `GeneralSimEnv` to `UnifiedSimEnv`.

## Algorithm Categories

### 1. SB3 Online RL Algorithms

These algorithms use Stable-Baselines3 and train online with environment interaction.

| Algorithm | Module | SB3 Package |
|-----------|--------|-------------|
| PPO | `algorithms/ppo.py` | stable-baselines3 |
| A2C | `algorithms/a2c.py` | stable-baselines3 |
| DQN | `algorithms/dqn.py` | stable-baselines3 |
| QR-DQN | `algorithms/qr_dqn.py` | sb3-contrib |

### 2. Tabular/Classical RL Algorithms

These algorithms use custom implementations without deep learning.

| Algorithm | Module | Notes |
|-----------|--------|-------|
| Q-Learning | `algorithms/q_learning.py` | Tabular, discrete state |
| Epsilon-Greedy Bandit | `algorithms/bandits.py` | Multi-armed bandit |
| UCB Bandit | `algorithms/bandits.py` | Upper confidence bound |

### 3. Offline RL Algorithms

These algorithms train on pre-collected datasets without environment interaction.

| Algorithm | Module | Notes |
|-----------|--------|-------|
| BC | `policies/bc_policy.py` | Behavior cloning (supervised) |
| IQL | `policies/iql_policy.py` | Implicit Q-learning |

---

## Migration Guide by Algorithm

### PPO (Proximal Policy Optimization)

**Legacy Usage:**
```python
from stable_baselines3 import PPO
from fusion.modules.rl.gymnasium_envs import SimEnv

env = SimEnv(sim_dict={"s1": config})
model = PPO("MultiInputPolicy", env, verbose=1)
model.learn(total_timesteps=100000)
```

**Unified Usage:**
```python
from sb3_contrib import MaskablePPO
from fusion.rl.environments import UnifiedSimEnv, ActionMaskWrapper

env = UnifiedSimEnv(config)
env = ActionMaskWrapper(env)

model = MaskablePPO("MultiInputPolicy", env, verbose=1)
model.learn(total_timesteps=100000)
```

**Migration Steps:**
1. Replace `SimEnv` with `UnifiedSimEnv`
2. Wrap with `ActionMaskWrapper` for action masking
3. Replace `PPO` with `MaskablePPO` from sb3-contrib
4. Use `model.predict(obs, action_masks=env.action_masks())` during inference
5. Retrain model (pre-trained models incompatible due to observation changes)

**Key Differences:**
- Action masking prevents invalid actions (faster learning)
- `paths_cong` renamed to `congestion` in observation
- New `is_feasible` observation feature
- Feasibility computed from real spectrum pipeline

**Retraining Required:** Recommended (action masking improves performance)

---

### A2C (Advantage Actor-Critic)

**Legacy Usage:**
```python
from stable_baselines3 import A2C
env = SimEnv(sim_dict={"s1": config})
model = A2C("MultiInputPolicy", env)
```

**Unified Usage:**
```python
from sb3_contrib import MaskableA2C  # If available, otherwise standard A2C
from fusion.rl.environments import UnifiedSimEnv, ActionMaskWrapper

env = UnifiedSimEnv(config)
env = ActionMaskWrapper(env)
model = MaskableA2C("MultiInputPolicy", env)  # or standard A2C
```

**Migration Steps:**
Same as PPO. Note: MaskableA2C may not be in sb3-contrib; use standard A2C with custom action selection if needed.

**Retraining Required:** Recommended

---

### DQN (Deep Q-Network)

**Legacy Usage:**
```python
from stable_baselines3 import DQN
env = SimEnv(sim_dict={"s1": config})
model = DQN("MultiInputPolicy", env)
```

**Unified Usage:**
```python
from stable_baselines3 import DQN
from fusion.rl.environments import UnifiedSimEnv

env = UnifiedSimEnv(config)
# DQN handles discrete actions natively

# Custom action masking during inference
model = DQN("MultiInputPolicy", env)

# During inference, manually apply mask
def masked_predict(model, obs, action_mask):
    q_values = model.policy.q_net(obs)
    q_values[~action_mask] = float("-inf")
    return q_values.argmax()
```

**Migration Steps:**
1. Replace `SimEnv` with `UnifiedSimEnv`
2. Implement custom action masking for DQN (no built-in support)
3. Retrain model

**Key Differences:**
- DQN does not have native action masking in SB3
- Requires custom masking wrapper or post-processing

**Retraining Required:** Required

---

### QR-DQN (Quantile Regression DQN)

**Legacy Usage:**
```python
from sb3_contrib import QRDQN
env = SimEnv(sim_dict={"s1": config})
model = QRDQN("MultiInputPolicy", env)
```

**Unified Usage:**
```python
from sb3_contrib import QRDQN
from fusion.rl.environments import UnifiedSimEnv

env = UnifiedSimEnv(config)
model = QRDQN("MultiInputPolicy", env)
```

**Migration Steps:**
Same as DQN. Custom action masking required.

**Retraining Required:** Required

---

### Q-Learning (Tabular)

**Legacy Usage:**
```python
from fusion.modules.rl.algorithms import QLearning

q_agent = QLearning(
    num_states=num_states,
    num_actions=k_paths,
    learning_rate=0.1,
    discount_factor=0.99,
)

# In environment step
action = q_agent.select_action(state_index)
q_agent.update(state_index, action, reward, next_state_index)
```

**Unified Usage:**
```python
from fusion.modules.rl.algorithms import QLearning
from fusion.rl.environments import UnifiedSimEnv

env = UnifiedSimEnv(config)
q_agent = QLearning(num_states=num_states, num_actions=k_paths, ...)

# Need state discretizer for continuous observations
discretizer = StateDiscretizer(env.observation_space, num_bins=10)

obs, info = env.reset()
while True:
    state_index = discretizer.discretize(obs)
    action_mask = info["action_mask"]

    # Select action with masking
    action = q_agent.select_action_masked(state_index, action_mask)

    obs, reward, term, trunc, info = env.step(action)
    next_state_index = discretizer.discretize(obs)

    q_agent.update(state_index, action, reward, next_state_index)

    if term or trunc:
        break
```

**Migration Steps:**
1. Create `StateDiscretizer` for continuous observations
2. Add `select_action_masked()` method to QLearning
3. Handle action masking in Q-value updates

**Key Differences:**
- Observation space is Dict (continuous) vs. discrete index
- Need discretization layer
- Action masking changes exploration strategy

**Retraining Required:** Required (different state representation)

---

### Bandits (Epsilon-Greedy, UCB)

**Legacy Usage:**
```python
from fusion.modules.rl.algorithms import EpsilonGreedyBandit

bandit = EpsilonGreedyBandit(num_arms=k_paths, epsilon=0.1)
action = bandit.select_arm()
bandit.update(action, reward)
```

**Unified Usage:**
```python
from fusion.modules.rl.algorithms import EpsilonGreedyBandit
from fusion.rl.environments import UnifiedSimEnv

env = UnifiedSimEnv(config)
bandit = EpsilonGreedyBandit(num_arms=k_paths, epsilon=0.1)

obs, info = env.reset()
while True:
    action_mask = info["action_mask"]

    # Select arm with masking
    action = bandit.select_arm_masked(action_mask)

    obs, reward, term, trunc, info = env.step(action)
    bandit.update(action, reward)

    if term or trunc:
        break
```

**Migration Steps:**
1. Add `select_arm_masked()` method to bandit classes
2. Use action mask from info dict

**Key Differences:**
- Action masking limits available arms
- Arm statistics should only update for valid arms

**Retraining Required:** Required (arm statistics reset)

---

### BC (Behavior Cloning)

**Legacy Usage:**
```python
from fusion.modules.rl.policies.bc_policy import BCPolicy

policy = BCPolicy("models/bc_model.pt", device="cpu")
action = policy.select_path(state, action_mask)
```

**Unified Usage:**
```python
from fusion.modules.rl.policies.bc_policy import BCPolicy
from fusion.rl.adapter import OfflinePolicyAdapter
from fusion.rl.environments import UnifiedSimEnv

env = UnifiedSimEnv(config)
policy = BCPolicy("models/bc_model.pt", device="cpu")
adapter = OfflinePolicyAdapter(policy, env.adapter, config)

obs, info = env.reset()
while True:
    request = env.current_request
    options = env.adapter._current_options
    network_state = env.network_state

    action = adapter.select_action(
        obs, info, request, options, network_state
    )

    obs, reward, term, trunc, info = env.step(action)

    if term or trunc:
        break
```

**Migration Steps:**
1. Use `OfflinePolicyAdapter` to bridge state formats
2. Access internal env state for offline policy
3. Keep existing BC model (adapter handles conversion)

**Key Differences:**
- State format conversion handled by adapter
- No retraining needed if using adapter
- Direct model usage requires state format changes

**Retraining Required:** Not required (use adapter)

---

### IQL (Implicit Q-Learning)

**Legacy Usage:**
```python
from fusion.modules.rl.policies.iql_policy import IQLPolicy

policy = IQLPolicy("models/iql_model.pt", device="cpu")
action = policy.select_path(state, action_mask)
```

**Unified Usage:**
Same as BC - use `OfflinePolicyAdapter`.

**Migration Steps:**
Same as BC.

**Retraining Required:** Not required (use adapter)

---

## Migration Decision Matrix

| Algorithm | Adapter Available | Retraining Needed | Action Masking | Complexity |
|-----------|------------------|-------------------|----------------|------------|
| PPO | N/A (online) | Recommended | MaskablePPO | Low |
| A2C | N/A (online) | Recommended | Custom/MaskableA2C | Low |
| DQN | N/A (online) | Required | Custom | Medium |
| QR-DQN | N/A (online) | Required | Custom | Medium |
| Q-Learning | N/A (tabular) | Required | Custom | High |
| Bandits | N/A (tabular) | Required | Custom | Low |
| BC | OfflinePolicyAdapter | No | Built-in | Low |
| IQL | OfflinePolicyAdapter | No | Built-in | Low |

---

## Common Migration Utilities

### Action Masking for Non-Maskable Algorithms

```python
class ActionMaskingWrapper:
    """Utility for algorithms without native action masking."""

    def __init__(self, policy, k_paths: int):
        self._policy = policy
        self._k_paths = k_paths

    def predict_with_mask(
        self,
        obs: np.ndarray,
        action_mask: np.ndarray,
    ) -> int:
        """Predict action with masking.

        Args:
            obs: Observation
            action_mask: Boolean mask of valid actions

        Returns:
            Selected action (masked)
        """
        # Get raw action/values from policy
        raw_output = self._policy.predict(obs)

        # Apply mask
        if isinstance(raw_output, np.ndarray):
            # Q-values or logits
            masked = raw_output.copy()
            masked[~action_mask] = float("-inf")
            return int(np.argmax(masked))
        else:
            # Direct action prediction
            action = raw_output
            if action_mask[action]:
                return action
            # Fallback to first valid action
            valid = np.where(action_mask)[0]
            return valid[0] if len(valid) > 0 else 0
```

### State Discretizer for Tabular Methods

```python
class StateDiscretizer:
    """Discretize continuous observations for tabular methods."""

    def __init__(
        self,
        observation_space: spaces.Dict,
        num_bins: int = 10,
    ):
        self._space = observation_space
        self._num_bins = num_bins
        self._setup_bins()

    def _setup_bins(self) -> None:
        """Create bin edges for each feature."""
        self._bins = {}
        for key, space in self._space.spaces.items():
            if isinstance(space, spaces.Box):
                low = space.low.flatten()
                high = space.high.flatten()
                self._bins[key] = [
                    np.linspace(l, h, self._num_bins + 1)
                    for l, h in zip(low, high)
                ]

    def discretize(self, obs: dict[str, np.ndarray]) -> int:
        """Convert observation dict to single state index."""
        indices = []
        for key in sorted(obs.keys()):
            if key in self._bins:
                for i, val in enumerate(obs[key].flatten()):
                    idx = np.digitize(val, self._bins[key][i]) - 1
                    idx = max(0, min(idx, self._num_bins - 1))
                    indices.append(idx)

        # Combine indices into single state
        state = 0
        for idx in indices:
            state = state * self._num_bins + idx
        return state
```

## Verification Checklist

- [ ] PPO migration tested and documented
- [ ] A2C migration tested and documented
- [ ] DQN migration tested with custom masking
- [ ] QR-DQN migration tested with custom masking
- [ ] Q-Learning migration with discretizer tested
- [ ] Bandit migration with arm masking tested
- [ ] BC migration with adapter tested
- [ ] IQL migration with adapter tested
- [ ] All examples run without errors
- [ ] Performance parity verified

## Next Task

Proceed to **P4.3.f: RLZoo3 Integration Migration**.
