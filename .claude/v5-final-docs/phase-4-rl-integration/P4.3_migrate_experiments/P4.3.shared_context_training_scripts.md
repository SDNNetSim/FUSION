# P4.3 Shared Context - Training Scripts and Experiments

**Purpose:** Document existing RL training infrastructure to inform migration planning.

## Source Files (for reference)

- `fusion/cli/run_train.py` - CLI entry point for training
- `fusion/sim/train_pipeline.py` - Training pipeline orchestration
- `fusion/modules/rl/workflow_runner.py` - RL workflow execution
- `fusion/modules/rl/utils/gym_envs.py` - Environment creation utilities
- `scripts/` - Training scripts directory
- `experiments/` - Experiment configurations

---

## 1. Training Entry Points

### CLI Entry Point

```python
# fusion/cli/run_train.py
def main() -> int:
    training_arguments = create_training_argument_parser()
    run_training_pipeline(training_arguments)
    return SUCCESS_EXIT_CODE
```

### Training Pipeline

```python
# fusion/sim/train_pipeline.py
def train_rl_agent(config):
    config_path = config.get_args().config_path

    # Create environment
    env, sim_dict, callback_list = create_environment(config_path)

    # Run training
    flat_dict = sim_dict.get("s1", sim_dict)
    flat_dict["callback"] = callback_list
    workflow_runner.run(env=env, sim_dict=flat_dict, callback_list=callback_list)
```

---

## 2. Environment Creation

### Current Factory

```python
# fusion/modules/rl/utils/gym_envs.py
def create_environment(config_path: str | None = None):
    # Setup callbacks
    ep_call_obj = EpisodicRewardCallback(verbose=1)
    param_call_obj = LearnRateEntCallback(verbose=1)
    callback_list = CallbackList([ep_call_obj, param_call_obj])

    # Load configuration
    flat_dict = setup_rl_sim(config_path=config_path)

    # Wrap dict
    wrapped_dict = {"s1": flat_dict} if "s1" not in flat_dict else flat_dict

    # Create environment (ALWAYS GeneralSimEnv currently)
    env = SimEnv(render_mode=None, custom_callback=ep_call_obj, sim_dict=wrapped_dict)

    return env, wrapped_dict, callback_list
```

### Environment Creation in Workflow

```python
# fusion/modules/rl/workflow_runner.py
def run(env: SimEnv, sim_dict: dict, callback_list: CallbackList):
    if sim_dict["is_training"]:
        if sim_dict.get("path_algorithm") in VALID_DRL_ALGORITHMS:
            _run_drl_training(env=env, sim_dict=sim_dict, yaml_dict=yaml_dict)
        else:
            # Legacy non-DRL training
            # ...
```

---

## 3. Supported Algorithms

### DRL Algorithms (via SB3)

| Algorithm | Import | Model Class |
|-----------|--------|-------------|
| PPO | `stable_baselines3` | `PPO` |
| A2C | `stable_baselines3` | `A2C` |
| DQN | `stable_baselines3` | `DQN` |
| QR-DQN | `sb3_contrib` | `QRDQN` |

### Legacy Algorithms (custom)

| Algorithm | Implementation |
|-----------|----------------|
| Q-Learning | `fusion/modules/rl/algorithms/q_learning.py` |
| Epsilon-Greedy | `fusion/modules/rl/algorithms/bandits.py` |
| UCB Bandit | `fusion/modules/rl/algorithms/bandits.py` |

---

## 4. Training Configuration

### Config File Keys

```ini
[rl]
is_training = true
path_algorithm = PPO
num_episodes = 1000
learning_rate = 0.0003
n_steps = 2048
batch_size = 64
gamma = 0.99
reward = 1.0
penalty = -1.0

[simulation]
k_paths = 3
spectral_slots = 320
num_requests = 1000
```

### Environment Variables

Currently no feature flags. Migration will add:
- `USE_UNIFIED_ENV` - Switch to new environment
- `RL_ENV_TYPE` - Explicit environment selection

---

## 5. Callback System

### Existing Callbacks

```python
# fusion/modules/rl/utils/callbacks.py

class EpisodicRewardCallback(BaseCallback):
    """Tracks episodic rewards."""
    def _on_step(self) -> bool:
        # Record episode rewards
        return True

class LearnRateEntCallback(BaseCallback):
    """Adjusts learning rate and entropy."""
    def _on_step(self) -> bool:
        # Adjust hyperparameters
        return True
```

### Callback Integration

```python
model = PPO("MultiInputPolicy", env, verbose=1)
model.learn(
    total_timesteps=total_timesteps,
    callback=callback_list,
)
```

---

## 6. Model Save/Load

### Current Pattern

```python
# Training
model = PPO("MultiInputPolicy", env)
model.learn(total_timesteps=100000)
model.save("ppo_optical_network")

# Inference
model = PPO.load("ppo_optical_network")
obs, info = env.reset()
action, _ = model.predict(obs)
```

### Model Compatibility

Models trained with `GeneralSimEnv` may not work with `UnifiedSimEnv` if observation spaces differ. Migration guide should address this.

---

## 7. Experiment Structure

### Typical Experiment

```
experiments/
├── nsfnet_ppo/
│   ├── config.ini           # Simulation config
│   ├── hyperparams.yaml     # RL hyperparameters
│   ├── train.py             # Training script
│   └── results/             # Output directory
│       ├── model.zip
│       ├── metrics.csv
│       └── tensorboard/
```

### Experiment Scripts

```python
# experiments/nsfnet_ppo/train.py
from fusion.modules.rl.utils.gym_envs import create_environment
from stable_baselines3 import PPO

def main():
    env, _, callbacks = create_environment("config.ini")

    model = PPO(
        "MultiInputPolicy",
        env,
        learning_rate=3e-4,
        n_steps=2048,
        verbose=1,
    )

    model.learn(total_timesteps=500_000, callback=callbacks)
    model.save("results/model")

if __name__ == "__main__":
    main()
```

---

## 8. Integration Points for Migration

### Factory Function Location

```python
# fusion/modules/rl/gymnasium_envs/__init__.py

# Current (implicit SimEnv)
from fusion.modules.rl.gymnasium_envs.general_sim_env import SimEnv

# After migration (factory pattern)
def create_sim_env(
    config: dict | SimulationConfig,
    use_unified: bool | None = None,
) -> gym.Env:
    """Create RL environment.

    Args:
        config: Simulation configuration
        use_unified: If True, use UnifiedSimEnv. If None, check env var.
    """
```

### Feature Flag Check Points

1. `create_environment()` in `gym_envs.py`
2. `workflow_runner.run()` for environment type
3. CLI argument parsing in `run_train.py`

---

## 9. Dependencies to Preserve

### SB3 Integration

Both environments must:
- Work with `PPO`, `A2C`, `DQN`, `QRDQN`
- Support callbacks
- Support model save/load
- Support tensorboard logging

### Legacy Algorithm Support

Legacy algorithms may need adaptation for `UnifiedSimEnv` or continued use with `GeneralSimEnv`.

---

## 10. Migration Risk Areas

| Area | Risk | Mitigation |
|------|------|------------|
| Observation space | May differ between envs | Document differences, provide adapter |
| Model compatibility | Old models may not work | Retrain or provide conversion |
| Training scripts | May assume GeneralSimEnv | Factory pattern with backward compat |
| Performance | UnifiedSimEnv may be slower | Benchmark and optimize |
| Behavior | May differ due to real pipelines | Parity validation (P4.4) |
