# Task ID: P5.3.a - Context Extraction: ML Policy Patterns

**Sub-phase:** P5.3
**Scope:** Phase 5 - ML Control + Protection only
**Task type:** context-extraction

---

## Purpose

Extract and document existing ML policy patterns from the legacy codebase to inform the design of MLControlPolicy. Focus on model loading, feature extraction, and inference patterns.

---

## Context to load before running this task

- `fusion/modules/rl/policies/bc_policy.py` - Behavior Cloning policy
- `fusion/modules/rl/policies/iql_policy.py` - Implicit Q-Learning policy
- `fusion/modules/rl/gymnasium_envs/general_sim_env.py` - RL environment (observation space)
- `.claude/v5-final-docs/phase-5-ml-protection/P5.3_ml_policy_support/P5.3.shared_context_model_formats.md`

---

## Outputs

### 1. BCPolicy Implementation Analysis

```python
# From fusion/modules/rl/policies/bc_policy.py

class BCPolicy(PathPolicy):
    """
    Behavior Cloning Policy.

    Loads a neural network trained to imitate expert (heuristic) behavior.
    The model is trained offline on demonstrations from KSP-FF or similar.
    """

    def __init__(
        self,
        model_path: str,
        device: str = "cpu",
        k_paths: int = 5,
    ) -> None:
        """
        Initialize BC policy.

        Args:
            model_path: Path to saved PyTorch model
            device: Inference device ('cpu' or 'cuda')
            k_paths: Number of candidate paths (must match training)
        """
        self.device = device
        self.k_paths = k_paths
        self.model = self._load_model(model_path)

    def _load_model(self, path: str) -> torch.nn.Module:
        """Load and prepare model for inference."""
        model = torch.load(path, map_location=self.device)
        model.eval()
        return model

    def _extract_features(self, state: dict[str, Any]) -> np.ndarray:
        """
        Convert state dictionary to feature vector.

        Features per path:
        - path_hops (normalized)
        - min_residual_slots (normalized)
        - frag_indicator (0-1)
        - failure_mask (0 or 1)
        """
        features = []

        # Global features
        features.append(state['slots_needed'] / 100.0)
        features.append(float(state.get('is_disaster', 0)))

        # Per-path features
        for path_info in state['paths']:
            features.extend([
                path_info['path_hops'] / 10.0,
                path_info['min_residual_slots'] / 100.0,
                path_info['frag_indicator'],
                float(path_info.get('failure_mask', 0)),
            ])

        return np.array(features, dtype=np.float32)

    def select_path(self, state: dict[str, Any], action_mask: list[bool]) -> int:
        """Select path using BC model."""
        features = self._extract_features(state)

        with torch.no_grad():
            tensor = torch.FloatTensor(features).to(self.device)
            logits = self.model(tensor)

        # Apply action mask
        masked_logits = logits.clone()
        for i, valid in enumerate(action_mask):
            if not valid:
                masked_logits[i] = float('-inf')

        # Select action
        if torch.all(torch.isinf(masked_logits)):
            return -1

        return int(torch.argmax(masked_logits).item())
```

**Key Observations:**
- Model loaded once at initialization
- Features extracted from state dict
- Action mask applied post-inference
- Returns -1 when all actions masked

### 2. IQLPolicy Implementation Analysis

```python
# From fusion/modules/rl/policies/iql_policy.py

class IQLPolicy(PathPolicy):
    """
    Implicit Q-Learning Policy.

    Uses a Q-network trained via conservative offline RL.
    Selects action with highest Q-value among valid actions.
    """

    def __init__(
        self,
        model_path: str,
        device: str = "cpu",
        k_paths: int = 5,
    ) -> None:
        self.device = device
        self.k_paths = k_paths
        self.q_network = self._load_model(model_path)

    def _load_model(self, path: str) -> torch.nn.Module:
        model = torch.load(path, map_location=self.device)
        model.eval()
        return model

    def _extract_features(self, state: dict[str, Any]) -> np.ndarray:
        """Same feature extraction as BCPolicy."""
        # ... identical implementation ...

    def select_path(self, state: dict[str, Any], action_mask: list[bool]) -> int:
        """Select path with highest Q-value."""
        features = self._extract_features(state)

        with torch.no_grad():
            tensor = torch.FloatTensor(features).to(self.device)
            q_values = self.q_network(tensor)

        # Apply mask and select argmax
        masked_q = q_values.clone()
        for i, valid in enumerate(action_mask):
            if not valid:
                masked_q[i] = float('-inf')

        if torch.all(torch.isinf(masked_q)):
            return -1

        return int(torch.argmax(masked_q).item())
```

**Key Observations:**
- Nearly identical structure to BCPolicy
- Only difference is semantic (Q-values vs policy logits)
- Same masking and selection logic

### 3. Observation Space Analysis

From `fusion/modules/rl/gymnasium_envs/general_sim_env.py`:

```python
class GeneralSimEnv(gym.Env):
    """General simulation environment for RL training."""

    def __init__(self, config: dict, k_paths: int = 5):
        self.k_paths = k_paths

        # Observation space definition
        # Shape: [1 + k_paths * 4] for slots + per-path features
        obs_dim = 2 + self.k_paths * 4  # Global + path features
        self.observation_space = spaces.Box(
            low=0.0,
            high=1.0,
            shape=(obs_dim,),
            dtype=np.float32,
        )

        # Action space: discrete selection among k paths
        self.action_space = spaces.Discrete(self.k_paths)

    def _get_observation(self) -> np.ndarray:
        """Build observation vector from current state."""
        obs = []

        # Global features
        obs.append(self.current_request.slots_needed / 100.0)
        obs.append(float(self.is_disaster))

        # Per-path features
        for i in range(self.k_paths):
            if i < len(self.current_paths):
                path_info = self.path_features[i]
                obs.extend([
                    path_info['hops'] / 10.0,
                    path_info['residual_slots'] / 100.0,
                    path_info['fragmentation'],
                    float(path_info['failure_affected']),
                ])
            else:
                # Padding for missing paths
                obs.extend([0.0, 0.0, 0.0, 0.0])

        return np.array(obs, dtype=np.float32)
```

**Key Observations:**
- Fixed-size observation vector
- Padding for variable number of paths
- All features normalized to [0, 1]
- Matches feature extraction in policies

### 4. Feature Mapping: State Dict to PathOption

| State Dict Field | PathOption Field | Normalization |
|------------------|------------------|---------------|
| `state['slots_needed']` | `request.bandwidth_gbps` | / 1000.0 |
| `path['path_hops']` | `len(opt.path) - 1` | / 10.0 |
| `path['min_residual_slots']` | (from network_state) | / 100.0 |
| `path['frag_indicator']` | (from network_state) | Already 0-1 |
| `path['failure_mask']` | (from network_state) | 0 or 1 |
| N/A | `opt.weight_km` | / 10000.0 |
| N/A | `opt.congestion` | Already 0-1 |
| N/A | `opt.is_feasible` | 0 or 1 |
| N/A | `opt.slots_needed` | / 100.0 |

### 5. Error Handling Patterns

Current policies have minimal error handling:

```python
# Current approach - no error handling
def select_path(self, state, action_mask) -> int:
    features = self._extract_features(state)  # May raise
    with torch.no_grad():
        tensor = torch.FloatTensor(features).to(self.device)
        output = self.model(tensor)  # May raise
    # ... selection logic
```

**Needed for MLControlPolicy:**

```python
# Robust approach with fallback
def select_action(self, request, options, network_state) -> int:
    try:
        features = self._build_features(request, options, network_state)
        output = self._predict(features)
        action = self._select_with_mask(output, options)

        if self._is_valid_action(action, options):
            return action

        logger.warning("Invalid action from model, using fallback")
        return self.fallback.select_action(request, options, network_state)

    except Exception as e:
        logger.warning(f"Model error: {e}, using fallback")
        return self.fallback.select_action(request, options, network_state)
```

### 6. Gap Analysis: Legacy vs MLControlPolicy

| Feature | BCPolicy/IQLPolicy | MLControlPolicy |
|---------|-------------------|-----------------|
| **Input format** | State dict + mask | Request + PathOption list |
| **Framework** | PyTorch only | PyTorch, sklearn, ONNX |
| **Fallback** | None | Configurable heuristic |
| **Error handling** | Minimal | Comprehensive with logging |
| **Protocol** | PathPolicy | ControlPolicy |
| **update()** | None | No-op method |
| **Device handling** | Basic | CUDA availability check |

### 7. Migration Requirements

To create MLControlPolicy from existing patterns:

1. **Replace PathPolicy with ControlPolicy** - Add update() method
2. **Replace state dict with Request/PathOption** - New feature extraction
3. **Add multi-framework support** - PyTorch, sklearn, ONNX loaders
4. **Add fallback mechanism** - Graceful degradation on errors
5. **Add comprehensive error handling** - Try/except with logging
6. **Add device validation** - Check CUDA availability

---

## Verification

- [ ] BCPolicy implementation documented
- [ ] IQLPolicy implementation documented
- [ ] Observation space from RL environment documented
- [ ] Feature mapping between state dict and PathOption completed
- [ ] Error handling patterns identified
- [ ] Gap analysis completed
- [ ] Migration requirements listed

---

## Next Task

Proceed to **P5.3.b** to design the MLControlPolicy class.
