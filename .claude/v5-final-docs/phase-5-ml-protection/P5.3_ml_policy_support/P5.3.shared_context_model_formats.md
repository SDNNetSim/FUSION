# Shared Context: Model Formats and Loading

**Purpose:** Document ML model formats, loading patterns, and inference methods
**Used by:** P5.3.a, P5.3.b, P5.3.c, P5.3.d, P5.3.e tasks

---

## Sources

- `fusion/modules/rl/policies/bc_policy.py` - Behavior Cloning policy
- `fusion/modules/rl/policies/iql_policy.py` - Implicit Q-Learning policy
- PyTorch documentation for model loading
- sklearn/joblib documentation for serialization
- ONNX Runtime documentation for inference

---

## Supported Model Frameworks

### PyTorch Models

**File Extensions:** `.pt`, `.pth`

**Loading:**
```python
import torch

def load_pytorch_model(path: str, device: str = "cpu") -> torch.nn.Module:
    """Load PyTorch model from file."""
    model = torch.load(path, map_location=device)
    model.eval()  # Set to evaluation mode
    return model
```

**Inference:**
```python
def predict_pytorch(model: torch.nn.Module, features: np.ndarray, device: str) -> np.ndarray:
    """Run inference with PyTorch model."""
    with torch.no_grad():
        tensor = torch.FloatTensor(features).unsqueeze(0).to(device)
        output = model(tensor)
        return output.cpu().numpy()[0]
```

**Expected Output:** Logits or Q-values for each action (shape: [K])

### scikit-learn Models

**File Extensions:** `.joblib`, `.pkl`

**Loading:**
```python
import joblib

def load_sklearn_model(path: str):
    """Load sklearn model from file."""
    return joblib.load(path)
```

**Inference:**
```python
def predict_sklearn(model, features: np.ndarray) -> np.ndarray:
    """Run inference with sklearn model."""
    # Reshape for single sample
    X = features.reshape(1, -1)

    # Use predict_proba if available (classifiers)
    if hasattr(model, 'predict_proba'):
        return model.predict_proba(X)[0]

    # Otherwise use predict (regressors)
    return model.predict(X)
```

**Expected Output:** Probabilities or values for each action

### ONNX Models

**File Extensions:** `.onnx`

**Loading:**
```python
import onnxruntime as ort

def load_onnx_model(path: str) -> ort.InferenceSession:
    """Load ONNX model as inference session."""
    return ort.InferenceSession(path)
```

**Inference:**
```python
def predict_onnx(session: ort.InferenceSession, features: np.ndarray) -> np.ndarray:
    """Run inference with ONNX model."""
    input_name = session.get_inputs()[0].name
    X = features.reshape(1, -1).astype(np.float32)
    output = session.run(None, {input_name: X})
    return output[0][0]
```

**Expected Output:** Model output tensor (varies by model architecture)

---

## Feature Vector Specification

### Observation Space (RL Compatible)

The feature vector must be compatible with RL training environments:

```python
# Feature vector construction
def build_features(
    request: Request,
    options: list[PathOption],
    network_state: NetworkState,
    k_paths: int = 5,
) -> np.ndarray:
    """Build feature vector for ML model."""
    features = []

    # Request-level features (global)
    features.append(request.bandwidth_gbps / 1000.0)  # Normalized

    # Path-level features (per path, padded to k_paths)
    for i in range(k_paths):
        if i < len(options):
            opt = options[i]
            features.extend([
                opt.weight_km / 10000.0,           # Normalized length
                opt.congestion,                     # Already 0-1
                1.0 if opt.is_feasible else 0.0,   # Feasibility
                (opt.slots_needed or 0) / 100.0,   # Normalized slots
            ])
        else:
            # Padding for missing paths
            features.extend([0.0, 1.0, 0.0, 0.0])

    return np.array(features, dtype=np.float32)
```

### Feature Vector Layout

For K=5 paths:

| Index | Feature | Description | Range |
|-------|---------|-------------|-------|
| 0 | bandwidth | Request bandwidth (normalized) | [0, 1] |
| 1 | path_0_length | Path 0 length (normalized) | [0, 1] |
| 2 | path_0_congestion | Path 0 congestion | [0, 1] |
| 3 | path_0_feasible | Path 0 feasibility flag | {0, 1} |
| 4 | path_0_slots | Path 0 slots (normalized) | [0, 1] |
| 5-8 | path_1_* | Path 1 features | ... |
| 9-12 | path_2_* | Path 2 features | ... |
| 13-16 | path_3_* | Path 3 features | ... |
| 17-20 | path_4_* | Path 4 features | ... |

**Total features:** 1 + (K * 4) = 21 for K=5

---

## Action Masking

### Mask Construction

```python
def build_action_mask(options: list[PathOption]) -> np.ndarray:
    """Build action mask from path feasibility."""
    return np.array([opt.is_feasible for opt in options], dtype=bool)
```

### Mask Application (Logits)

```python
def apply_mask_to_logits(logits: np.ndarray, mask: np.ndarray) -> np.ndarray:
    """Set infeasible action logits to -infinity."""
    masked = logits.copy()
    masked[~mask] = float('-inf')
    return masked
```

### Mask Application (Probabilities)

```python
def apply_mask_to_probs(probs: np.ndarray, mask: np.ndarray) -> np.ndarray:
    """Zero out infeasible action probabilities and renormalize."""
    masked = probs.copy()
    masked[~mask] = 0.0
    total = masked.sum()
    if total > 0:
        masked /= total
    return masked
```

### Action Selection

```python
def select_from_masked(output: np.ndarray, mask: np.ndarray) -> int:
    """Select best action respecting mask."""
    masked = apply_mask_to_logits(output, mask)

    # Check if any valid action exists
    if np.all(np.isinf(masked)):
        return -1

    return int(np.argmax(masked))
```

---

## Existing ML Policy Patterns

### BCPolicy (Behavior Cloning)

From `fusion/modules/rl/policies/bc_policy.py`:

```python
class BCPolicy(PathPolicy):
    """Behavior Cloning policy - learns from expert demonstrations."""

    def __init__(self, model_path: str, device: str = "cpu"):
        self.device = device
        self.model = self._load_model(model_path)

    def _load_model(self, path: str):
        model = torch.load(path, map_location=self.device)
        model.eval()
        return model

    def _extract_features(self, state: dict) -> np.ndarray:
        """Convert state dict to feature vector."""
        features = []
        features.append(state['slots_needed'] / 100.0)
        for path in state['paths']:
            features.extend([
                path['path_hops'] / 10.0,
                path['min_residual_slots'] / 100.0,
                path['frag_indicator'],
                path['failure_mask'],
            ])
        return np.array(features, dtype=np.float32)

    def select_path(self, state: dict, action_mask: list[bool]) -> int:
        features = self._extract_features(state)
        with torch.no_grad():
            tensor = torch.FloatTensor(features).to(self.device)
            logits = self.model(tensor)
        # Apply mask and select
        masked = self._apply_mask(logits, action_mask)
        return int(torch.argmax(masked).item())
```

### IQLPolicy (Implicit Q-Learning)

From `fusion/modules/rl/policies/iql_policy.py`:

```python
class IQLPolicy(PathPolicy):
    """Implicit Q-Learning policy - conservative offline RL."""

    # Similar structure to BCPolicy
    # Uses Q-network instead of policy network
    # Action selection via argmax Q-values
```

---

## Model Error Handling

### Expected Errors

| Error Type | Cause | Handling |
|------------|-------|----------|
| `FileNotFoundError` | Model file missing | Use fallback |
| `torch.serialization.pickle.UnpicklingError` | Corrupt model | Use fallback |
| `RuntimeError` | Shape mismatch | Use fallback, log warning |
| `ValueError` | Invalid input | Use fallback, log warning |
| `onnxruntime.OrtException` | ONNX inference error | Use fallback |

### Error Handling Pattern

```python
def select_action_safe(self, request, options, network_state) -> int:
    """Select action with error handling and fallback."""
    try:
        # Build features
        features = self._build_features(request, options, network_state)

        # Run model inference
        output = self._predict(features)

        # Apply mask and select
        mask = self._build_mask(options)
        action = self._select_from_output(output, mask)

        # Validate action
        if 0 <= action < len(options) and options[action].is_feasible:
            return action

        # Invalid action from model - use fallback
        logger.warning("ML model returned invalid action, using fallback")
        return self.fallback.select_action(request, options, network_state)

    except Exception as e:
        logger.warning(f"ML model error: {e}, using fallback")
        return self.fallback.select_action(request, options, network_state)
```

---

## Device Management

### Device Selection

```python
def get_device(device_str: str) -> str:
    """Resolve device string to actual device."""
    if device_str == "cuda" and torch.cuda.is_available():
        return "cuda"
    elif device_str == "cuda":
        logger.warning("CUDA requested but not available, using CPU")
        return "cpu"
    return "cpu"
```

### Device Consistency

Models and input tensors must be on the same device:

```python
# Bad - device mismatch
model = model.to("cuda")
tensor = torch.FloatTensor(features)  # On CPU!
output = model(tensor)  # Error!

# Good - consistent devices
model = model.to(device)
tensor = torch.FloatTensor(features).to(device)
output = model(tensor)  # Works
```

---

## Performance Considerations

### Batch vs Single Inference

MLControlPolicy processes one request at a time (single inference):

```python
# Single request inference
features = build_features(request, options, network_state)
output = model(features.unsqueeze(0))[0]  # Add batch dim, remove after
```

For batch processing (future optimization):

```python
# Batch inference (not implemented in P5.3)
batch_features = stack_features(requests, all_options)
outputs = model(batch_features)  # Process all at once
```

### Memory Management

```python
# Always use no_grad for inference
with torch.no_grad():
    output = model(tensor)

# Don't accumulate gradients
model.eval()  # Disables dropout, batchnorm training mode
```
