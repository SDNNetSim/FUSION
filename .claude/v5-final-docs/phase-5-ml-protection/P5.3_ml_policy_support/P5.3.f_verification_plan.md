# Task ID: P5.3.f - Verification Plan: ML Policy Support

**Sub-phase:** P5.3
**Scope:** Phase 5 - ML Control + Protection only
**Task type:** verification-plan

---

## Purpose

Define comprehensive unit tests for MLControlPolicy, FeatureBuilder, and supporting classes to ensure correct model loading, inference, fallback behavior, and feature engineering.

---

## Context to load before running this task

- `.claude/v5-final-docs/phase-5-ml-protection/P5.3_ml_policy_support/P5.3.e_implement_ml_policy.md`
- `fusion/policies/ml_policy.py`
- `fusion/rl/adapter.py` (PathOption)

---

## Outputs

### 1. Test File Structure

```
fusion/tests/
└── policies/
    ├── __init__.py
    ├── conftest.py                  # Shared fixtures (from P5.2)
    ├── test_heuristic_policy.py     # From P5.2
    └── test_ml_policy.py            # NEW: ML policy tests
```

### 2. Create `fusion/tests/policies/test_ml_policy.py`

```python
"""
Tests for MLControlPolicy and supporting classes.

Uses mock models to test without requiring actual ML frameworks.
"""

from __future__ import annotations

from pathlib import Path
from typing import Any
from unittest.mock import MagicMock, patch

import numpy as np
import pytest

from fusion.policies.ml_policy import FeatureBuilder, MLControlPolicy, ModelType
from fusion.rl.adapter import PathOption


# ============================================================================
# Mock Request for Testing
# ============================================================================


class MockRequest:
    """Mock Request object for testing."""

    def __init__(self, bandwidth_gbps: float = 100.0) -> None:
        self.bandwidth_gbps = bandwidth_gbps


# ============================================================================
# Test ModelType
# ============================================================================


class TestModelType:
    """Tests for ModelType enum."""

    def test_from_path_pytorch_pt(self) -> None:
        """Should identify .pt as PyTorch."""
        assert ModelType.from_path("model.pt") == ModelType.PYTORCH

    def test_from_path_pytorch_pth(self) -> None:
        """Should identify .pth as PyTorch."""
        assert ModelType.from_path("model.pth") == ModelType.PYTORCH

    def test_from_path_sklearn_joblib(self) -> None:
        """Should identify .joblib as sklearn."""
        assert ModelType.from_path("model.joblib") == ModelType.SKLEARN

    def test_from_path_sklearn_pkl(self) -> None:
        """Should identify .pkl as sklearn."""
        assert ModelType.from_path("model.pkl") == ModelType.SKLEARN

    def test_from_path_onnx(self) -> None:
        """Should identify .onnx as ONNX."""
        assert ModelType.from_path("model.onnx") == ModelType.ONNX

    def test_from_path_case_insensitive(self) -> None:
        """Should handle uppercase extensions."""
        assert ModelType.from_path("model.PT") == ModelType.PYTORCH
        assert ModelType.from_path("model.ONNX") == ModelType.ONNX

    def test_from_path_unknown_extension(self) -> None:
        """Should raise ValueError for unknown extension."""
        with pytest.raises(ValueError, match="Unknown model extension"):
            ModelType.from_path("model.h5")

    def test_from_path_with_directory(self) -> None:
        """Should handle paths with directories."""
        assert ModelType.from_path("/path/to/model.pt") == ModelType.PYTORCH
        assert ModelType.from_path("models/subdir/model.onnx") == ModelType.ONNX


# ============================================================================
# Test FeatureBuilder
# ============================================================================


class TestFeatureBuilder:
    """Tests for FeatureBuilder."""

    def test_feature_size_calculation(self) -> None:
        """Feature size should be 1 + 4*k_paths."""
        assert FeatureBuilder(k_paths=5).feature_size == 21
        assert FeatureBuilder(k_paths=3).feature_size == 13
        assert FeatureBuilder(k_paths=1).feature_size == 5

    def test_build_with_all_paths(self) -> None:
        """Should correctly extract features from full options list."""
        builder = FeatureBuilder(k_paths=2)
        request = MockRequest(bandwidth_gbps=500.0)
        options = [
            PathOption.from_unprotected_route(
                0, ["A", "B"], 2000.0, True, "QPSK", 20, 0.3
            ),
            PathOption.from_unprotected_route(
                1, ["A", "C"], 5000.0, False, None, None, 0.7
            ),
        ]

        features = builder.build(request, options, None)

        assert features.shape == (9,)
        assert features.dtype == np.float32

        # Request feature
        assert features[0] == pytest.approx(0.5)  # 500/1000

        # Path 0 features
        assert features[1] == pytest.approx(0.2)  # 2000/10000
        assert features[2] == pytest.approx(0.3)  # congestion
        assert features[3] == pytest.approx(1.0)  # feasible
        assert features[4] == pytest.approx(0.2)  # 20/100

        # Path 1 features
        assert features[5] == pytest.approx(0.5)  # 5000/10000
        assert features[6] == pytest.approx(0.7)  # congestion
        assert features[7] == pytest.approx(0.0)  # not feasible
        assert features[8] == pytest.approx(0.0)  # None -> 0

    def test_build_with_padding(self) -> None:
        """Should pad missing paths correctly."""
        builder = FeatureBuilder(k_paths=3)
        request = MockRequest(bandwidth_gbps=100.0)
        options = [
            PathOption.from_unprotected_route(
                0, ["A", "B"], 1000.0, True, "QPSK", 10, 0.5
            ),
        ]

        features = builder.build(request, options, None)

        assert features.shape == (13,)

        # Path 1 (padding)
        assert features[5] == 0.0  # weight padding
        assert features[6] == 1.0  # congestion padding (high)
        assert features[7] == 0.0  # feasible padding (not)
        assert features[8] == 0.0  # slots padding

        # Path 2 (padding)
        assert features[9] == 0.0
        assert features[10] == 1.0
        assert features[11] == 0.0
        assert features[12] == 0.0

    def test_build_empty_options(self) -> None:
        """Should handle empty options with all padding."""
        builder = FeatureBuilder(k_paths=2)
        request = MockRequest(bandwidth_gbps=200.0)

        features = builder.build(request, [], None)

        assert features.shape == (9,)
        assert features[0] == pytest.approx(0.2)  # bandwidth

        # All paths are padding
        for i in range(2):
            base = 1 + i * 4
            assert features[base] == 0.0  # weight
            assert features[base + 1] == 1.0  # congestion
            assert features[base + 2] == 0.0  # feasible
            assert features[base + 3] == 0.0  # slots


# ============================================================================
# Test MLControlPolicy
# ============================================================================


class TestMLControlPolicyInit:
    """Tests for MLControlPolicy initialization."""

    def test_file_not_found(self, tmp_path: Path) -> None:
        """Should raise FileNotFoundError for missing model."""
        with pytest.raises(FileNotFoundError, match="not found"):
            MLControlPolicy(str(tmp_path / "nonexistent.pt"))

    def test_infers_model_type_from_extension(self, tmp_path: Path) -> None:
        """Should infer model type from file extension."""
        # Create dummy files
        (tmp_path / "model.pt").touch()
        (tmp_path / "model.joblib").touch()
        (tmp_path / "model.onnx").touch()

        # Mock the model loading to avoid actual framework imports
        with patch.object(MLControlPolicy, '_load_model', return_value=MagicMock()):
            policy_pt = MLControlPolicy(str(tmp_path / "model.pt"))
            assert policy_pt.model_type == ModelType.PYTORCH

            policy_sklearn = MLControlPolicy(str(tmp_path / "model.joblib"))
            assert policy_sklearn.model_type == ModelType.SKLEARN

            policy_onnx = MLControlPolicy(str(tmp_path / "model.onnx"))
            assert policy_onnx.model_type == ModelType.ONNX

    def test_explicit_model_type(self, tmp_path: Path) -> None:
        """Should use explicit model type if provided."""
        (tmp_path / "model.bin").touch()

        with patch.object(MLControlPolicy, '_load_model', return_value=MagicMock()):
            policy = MLControlPolicy(
                str(tmp_path / "model.bin"),
                model_type="pytorch",
            )
            assert policy.model_type == ModelType.PYTORCH

    def test_default_fallback_is_first_feasible(self, tmp_path: Path) -> None:
        """Should use FirstFeasiblePolicy as default fallback."""
        (tmp_path / "model.pt").touch()

        with patch.object(MLControlPolicy, '_load_model', return_value=MagicMock()):
            policy = MLControlPolicy(str(tmp_path / "model.pt"))

            from fusion.policies.heuristic_policy import FirstFeasiblePolicy
            assert isinstance(policy.fallback, FirstFeasiblePolicy)

    def test_custom_fallback_type(self, tmp_path: Path) -> None:
        """Should create fallback from type string."""
        (tmp_path / "model.pt").touch()

        with patch.object(MLControlPolicy, '_load_model', return_value=MagicMock()):
            policy = MLControlPolicy(
                str(tmp_path / "model.pt"),
                fallback_type="least_congested",
            )

            from fusion.policies.heuristic_policy import LeastCongestedPolicy
            assert isinstance(policy.fallback, LeastCongestedPolicy)

    def test_custom_fallback_policy(self, tmp_path: Path) -> None:
        """Should use provided fallback policy."""
        (tmp_path / "model.pt").touch()

        from fusion.policies.heuristic_policy import ShortestFeasiblePolicy
        custom_fallback = ShortestFeasiblePolicy()

        with patch.object(MLControlPolicy, '_load_model', return_value=MagicMock()):
            policy = MLControlPolicy(
                str(tmp_path / "model.pt"),
                fallback_policy=custom_fallback,
            )

            assert policy.fallback is custom_fallback

    def test_invalid_fallback_type(self, tmp_path: Path) -> None:
        """Should raise ValueError for invalid fallback type."""
        (tmp_path / "model.pt").touch()

        with patch.object(MLControlPolicy, '_load_model', return_value=MagicMock()):
            with pytest.raises(ValueError, match="Unknown fallback type"):
                MLControlPolicy(
                    str(tmp_path / "model.pt"),
                    fallback_type="invalid_policy",
                )


class TestMLControlPolicySelectAction:
    """Tests for MLControlPolicy.select_action()."""

    @pytest.fixture
    def mock_policy(self, tmp_path: Path) -> MLControlPolicy:
        """Create policy with mocked model."""
        (tmp_path / "model.pt").touch()

        with patch.object(MLControlPolicy, '_load_model') as mock_load:
            mock_model = MagicMock()
            mock_load.return_value = mock_model

            policy = MLControlPolicy(str(tmp_path / "model.pt"))
            policy._mock_model = mock_model
            return policy

    def test_empty_options_returns_negative_one(self, mock_policy: MLControlPolicy) -> None:
        """Should return -1 for empty options."""
        action = mock_policy.select_action(MockRequest(), [], None)
        assert action == -1

    def test_uses_fallback_on_predict_error(self, mock_policy: MLControlPolicy) -> None:
        """Should use fallback when prediction raises."""
        # Make _predict raise
        with patch.object(mock_policy, '_predict', side_effect=RuntimeError("Model error")):
            options = [
                PathOption.from_unprotected_route(0, ["A", "B"], 100, True, "QPSK", 4, 0.5),
            ]

            action = mock_policy.select_action(MockRequest(), options, None)

            # Fallback (FirstFeasible) should return 0
            assert action == 0
            assert mock_policy._fallback_calls == 1

    def test_uses_fallback_on_invalid_action(self, mock_policy: MLControlPolicy) -> None:
        """Should use fallback when model selects infeasible action."""
        # Mock prediction returning index of infeasible option
        with patch.object(mock_policy, '_predict', return_value=np.array([0.9, 0.1])):
            options = [
                PathOption.from_unprotected_route(0, ["A", "B"], 100, False, None, None, 0.5),  # Infeasible
                PathOption.from_unprotected_route(1, ["A", "C"], 200, True, "QPSK", 4, 0.3),   # Feasible
            ]

            action = mock_policy.select_action(MockRequest(), options, None)

            # Model would select 0, but it's infeasible, so fallback selects 1
            assert action == 1
            assert mock_policy._fallback_calls == 1

    def test_selects_model_action_when_valid(self, mock_policy: MLControlPolicy) -> None:
        """Should return model's action when valid."""
        # Mock prediction preferring index 1
        with patch.object(mock_policy, '_predict', return_value=np.array([0.1, 0.9])):
            options = [
                PathOption.from_unprotected_route(0, ["A", "B"], 100, True, "QPSK", 4, 0.5),
                PathOption.from_unprotected_route(1, ["A", "C"], 200, True, "QPSK", 4, 0.3),
            ]

            action = mock_policy.select_action(MockRequest(), options, None)

            assert action == 1
            assert mock_policy._fallback_calls == 0

    def test_action_masking_applied(self, mock_policy: MLControlPolicy) -> None:
        """Should mask infeasible actions before selection."""
        # Mock prediction preferring index 0 (highest logit)
        with patch.object(mock_policy, '_predict', return_value=np.array([0.9, 0.5, 0.1])):
            options = [
                PathOption.from_unprotected_route(0, ["A", "B"], 100, False, None, None, 0.5),  # Infeasible
                PathOption.from_unprotected_route(1, ["A", "C"], 200, True, "QPSK", 4, 0.3),   # Feasible
                PathOption.from_unprotected_route(2, ["A", "D"], 300, True, "QPSK", 4, 0.7),   # Feasible
            ]

            action = mock_policy.select_action(MockRequest(), options, None)

            # Index 0 masked out, so should select 1 (next highest among feasible)
            assert action == 1


class TestMLControlPolicyStats:
    """Tests for MLControlPolicy statistics tracking."""

    @pytest.fixture
    def policy_with_mock(self, tmp_path: Path) -> MLControlPolicy:
        """Create policy with mocked model."""
        (tmp_path / "model.pt").touch()

        with patch.object(MLControlPolicy, '_load_model', return_value=MagicMock()):
            return MLControlPolicy(str(tmp_path / "model.pt"))

    def test_initial_stats(self, policy_with_mock: MLControlPolicy) -> None:
        """Should have zero stats initially."""
        stats = policy_with_mock.get_stats()
        assert stats["total_calls"] == 0
        assert stats["fallback_calls"] == 0
        assert stats["fallback_rate"] == 0.0
        assert stats["error_types"] == {}

    def test_stats_tracking(self, policy_with_mock: MLControlPolicy) -> None:
        """Should track calls and fallbacks."""
        policy = policy_with_mock

        # Simulate some calls
        policy._total_calls = 10
        policy._fallback_calls = 3
        policy._error_types = {"runtime_error": 2, "invalid_action": 1}

        stats = policy.get_stats()
        assert stats["total_calls"] == 10
        assert stats["fallback_calls"] == 3
        assert stats["fallback_rate"] == pytest.approx(0.3)
        assert stats["error_types"]["runtime_error"] == 2

    def test_reset_stats(self, policy_with_mock: MLControlPolicy) -> None:
        """Should reset all statistics."""
        policy = policy_with_mock
        policy._total_calls = 10
        policy._fallback_calls = 5
        policy._error_types = {"error": 1}

        policy.reset_stats()

        stats = policy.get_stats()
        assert stats["total_calls"] == 0
        assert stats["fallback_calls"] == 0
        assert stats["error_types"] == {}


class TestMLControlPolicyProtocol:
    """Tests for ControlPolicy protocol compliance."""

    @pytest.fixture
    def policy(self, tmp_path: Path) -> MLControlPolicy:
        """Create policy with mocked model."""
        (tmp_path / "model.pt").touch()

        with patch.object(MLControlPolicy, '_load_model', return_value=MagicMock()):
            return MLControlPolicy(str(tmp_path / "model.pt"))

    def test_has_select_action(self, policy: MLControlPolicy) -> None:
        """Should have select_action method."""
        assert hasattr(policy, 'select_action')
        assert callable(policy.select_action)

    def test_has_update(self, policy: MLControlPolicy) -> None:
        """Should have update method."""
        assert hasattr(policy, 'update')
        assert callable(policy.update)

    def test_update_is_noop(self, policy: MLControlPolicy) -> None:
        """update() should be a no-op."""
        # Should not raise
        policy.update(MockRequest(), 0, 1.0)
        policy.update(MockRequest(), -1, -0.5)

    def test_get_name(self, policy: MLControlPolicy) -> None:
        """Should return descriptive name."""
        name = policy.get_name()
        assert "MLControlPolicy" in name
        assert "pytorch" in name
```

---

## Verification Commands

```bash
# Run all ML policy tests
pytest fusion/tests/policies/test_ml_policy.py -v

# Run with coverage
pytest fusion/tests/policies/test_ml_policy.py \
    --cov=fusion/policies/ml_policy \
    --cov-report=term-missing

# Run specific test class
pytest fusion/tests/policies/test_ml_policy.py::TestFeatureBuilder -v

# Type checking
mypy fusion/tests/policies/test_ml_policy.py
```

---

## Coverage Requirements

| Component | Target | Notes |
|-----------|--------|-------|
| ModelType | 100% | Enum and from_path |
| FeatureBuilder | 95% | All feature construction |
| MLControlPolicy.__init__ | 90% | Initialization paths |
| MLControlPolicy.select_action | 90% | Main selection with fallback |
| MLControlPolicy stats | 100% | Statistics tracking |

---

## Verification Checklist

- [ ] ModelType enum tests complete
- [ ] FeatureBuilder tests cover all scenarios
- [ ] MLControlPolicy initialization tests
- [ ] Fallback mechanism tests
- [ ] Action masking tests
- [ ] Statistics tracking tests
- [ ] Protocol compliance tests
- [ ] Mock models used (no framework dependencies)
- [ ] All tests pass
- [ ] Coverage meets targets

---

## Next Sub-Phase

After completing P5.3, proceed to **P5.4** (Protection Pipeline) or **P5.5** (Orchestrator Integration).
