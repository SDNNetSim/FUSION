# Task ID: P5.1.e - Design RLPolicy Wrapper

**Sub-phase:** P5.1
**Scope:** Phase 5 - ML Control + Protection only
**Task type:** design

---

## Purpose

Design the `RLPolicy` wrapper class that enables existing Stable-Baselines3 models to implement the `ControlPolicy` protocol. This ensures backwards compatibility with Phase 4 RL integration while providing a unified policy interface.

---

## Context to load before running this task

- `.claude/v5-final-docs/phase-5-ml-protection/P5.1_control_policy_protocol/P5.1.b_design_control_policy_protocol.md`
- `.claude/v5-final-docs/phase-4-rl-integration/P4.1_rl_adapter/P4.1.index.md`
- `.claude/v4-docs/architecture/control_policy.md` (RLPolicy section)

---

## Outputs

### 1. Create `fusion/policies/rl_policy.py`

```python
"""
RL Policy wrapper for Stable-Baselines3 models.

This module provides the RLPolicy class that wraps pre-trained SB3 models
to implement the ControlPolicy protocol, enabling unified policy handling
in the SDNOrchestrator.
"""

from __future__ import annotations

import logging
from typing import TYPE_CHECKING, Any

import numpy as np

if TYPE_CHECKING:
    from stable_baselines3.common.base_class import BaseAlgorithm

    from fusion.domain.network_state import NetworkState
    from fusion.domain.request import Request
    from fusion.rl.adapter import PathOption, RLSimulationAdapter

logger = logging.getLogger(__name__)


class RLPolicy:
    """
    Wrapper enabling SB3 models to implement ControlPolicy.

    This adapter bridges the existing RL infrastructure with the
    ControlPolicy protocol. It handles:

    1. **Observation building**: Converts (request, options, state) to RL observation
    2. **Action masking**: Enforces feasibility constraints during prediction
    3. **Action conversion**: Converts SB3 action to path index

    The wrapped model is pre-trained and does not learn online. Use
    UnifiedSimEnv and SB3's learn() for online training.

    Attributes:
        model: Pre-trained SB3 model (PPO, DQN, A2C, etc.)
        adapter: Optional RLSimulationAdapter for observation building
        k_paths: Number of path options (for observation space)

    Example:
        >>> from stable_baselines3 import PPO
        >>> model = PPO.load("trained_model.zip")
        >>> policy = RLPolicy(model)
        >>> action = policy.select_action(request, options, network_state)
    """

    def __init__(
        self,
        model: BaseAlgorithm,
        adapter: RLSimulationAdapter | None = None,
        k_paths: int = 5,
    ) -> None:
        """
        Initialize RLPolicy with a trained SB3 model.

        Args:
            model: Pre-trained SB3 model with predict() method
            adapter: Optional adapter for observation building. If None,
                uses internal observation construction.
            k_paths: Expected number of path options (for obs space size)
        """
        self.model = model
        self._adapter = adapter
        self.k_paths = k_paths

        # Validate model has predict method
        if not hasattr(model, 'predict'):
            raise ValueError(
                f"Model {type(model).__name__} does not have predict() method"
            )

        logger.info(
            f"RLPolicy initialized with {type(model).__name__}, k_paths={k_paths}"
        )

    def select_action(
        self,
        request: Request,
        options: list[PathOption],
        network_state: NetworkState,
    ) -> int:
        """
        Select action using the trained SB3 model.

        Builds an observation from the inputs, generates an action mask
        from feasibility flags, and uses the model to predict an action.

        Args:
            request: The incoming request to serve
            options: Available path options with feasibility information
            network_state: Current state of the network

        Returns:
            Path index (0 to len(options)-1), or -1 if no valid action

        Note:
            The model must support action masking. For models trained with
            sb3-contrib's MaskablePPO, the action_masks parameter is used.
            For standard models, masking is applied post-prediction.
        """
        # Build observation
        obs = self._build_observation(request, options, network_state)

        # Build action mask
        action_mask = self._build_action_mask(options)

        # Check if any action is valid
        if not any(action_mask):
            logger.debug("No feasible actions available")
            return -1

        try:
            # Try to use native action masking if available
            if self._supports_action_masking():
                action, _ = self.model.predict(
                    obs,
                    deterministic=True,
                    action_masks=np.array(action_mask),
                )
            else:
                # Predict without masking, then validate
                action, _ = self.model.predict(obs, deterministic=True)
                action = int(action)

                # If predicted action is infeasible, find first feasible
                if action >= len(options) or not action_mask[action]:
                    logger.debug(
                        f"Model predicted infeasible action {action}, "
                        "selecting first feasible"
                    )
                    action = self._find_first_feasible(action_mask)

            return int(action) if action >= 0 else -1

        except Exception as e:
            logger.warning(f"Model prediction failed: {e}, returning -1")
            return -1

    def _build_observation(
        self,
        request: Request,
        options: list[PathOption],
        network_state: NetworkState,
    ) -> np.ndarray:
        """
        Build observation array for model prediction.

        If an adapter is available, delegates to adapter.build_observation().
        Otherwise, constructs observation matching training format.

        Returns:
            numpy array matching model's observation space
        """
        if self._adapter is not None:
            return self._adapter.build_observation(request, options, network_state)

        # Internal observation construction
        features = []

        # Request features
        features.append(request.bandwidth_gbps / 1000.0)  # Normalized

        # Per-path features (padded to k_paths)
        for i in range(self.k_paths):
            if i < len(options):
                opt = options[i]
                features.extend([
                    opt.weight_km / 10000.0,           # Normalized length
                    opt.congestion,                     # Already 0-1
                    1.0 if opt.is_feasible else 0.0,   # Feasibility
                    (opt.slots_needed or 0) / 100.0,   # Normalized slots
                ])
            else:
                # Padding for missing paths
                features.extend([0.0, 1.0, 0.0, 0.0])

        return np.array(features, dtype=np.float32)

    def _build_action_mask(self, options: list[PathOption]) -> list[bool]:
        """
        Build action mask from path options.

        Returns:
            List of booleans, True where action is valid (is_feasible)
        """
        mask = [opt.is_feasible for opt in options]

        # Pad to k_paths if needed
        while len(mask) < self.k_paths:
            mask.append(False)

        return mask[:self.k_paths]

    def _supports_action_masking(self) -> bool:
        """Check if model supports native action masking."""
        # MaskablePPO and similar algorithms support action_masks parameter
        model_name = type(self.model).__name__
        return model_name in ('MaskablePPO', 'MaskableRecurrentPPO')

    def _find_first_feasible(self, mask: list[bool]) -> int:
        """Find index of first feasible action."""
        for i, is_feasible in enumerate(mask):
            if is_feasible:
                return i
        return -1

    def update(self, request: Request, action: int, reward: float) -> None:
        """
        Update policy based on experience.

        RLPolicy wraps pre-trained models that do not learn online.
        This method is a no-op to satisfy the ControlPolicy protocol.

        For online RL training, use UnifiedSimEnv with SB3's learn() method.
        """
        pass

    def get_name(self) -> str:
        """
        Return policy name for logging and metrics.

        Returns:
            String identifying this policy and underlying model
        """
        model_name = type(self.model).__name__
        return f"RLPolicy({model_name})"

    def set_adapter(self, adapter: RLSimulationAdapter) -> None:
        """
        Set the RL simulation adapter for observation building.

        Args:
            adapter: Adapter from Phase 4 RL integration
        """
        self._adapter = adapter

    @classmethod
    def from_file(
        cls,
        model_path: str,
        algorithm: str = "PPO",
        **kwargs: Any,
    ) -> "RLPolicy":
        """
        Load RLPolicy from a saved model file.

        Args:
            model_path: Path to saved model (e.g., "model.zip")
            algorithm: SB3 algorithm name ("PPO", "DQN", "A2C", etc.)
            **kwargs: Additional arguments passed to RLPolicy.__init__

        Returns:
            RLPolicy wrapping the loaded model

        Example:
            >>> policy = RLPolicy.from_file("trained_ppo.zip", algorithm="PPO")
        """
        import importlib

        # Import algorithm class
        try:
            sb3_module = importlib.import_module("stable_baselines3")
            algorithm_class = getattr(sb3_module, algorithm)
        except (ImportError, AttributeError):
            # Try sb3_contrib for maskable algorithms
            try:
                sb3_contrib = importlib.import_module("sb3_contrib")
                algorithm_class = getattr(sb3_contrib, algorithm)
            except (ImportError, AttributeError):
                raise ValueError(f"Unknown algorithm: {algorithm}")

        model = algorithm_class.load(model_path)
        return cls(model, **kwargs)


# Type alias for backwards compatibility
RLControlPolicy = RLPolicy
```

### 2. Design Rationale Table

| Decision | Rationale |
|----------|-----------|
| **Wrap SB3 model** | Leverage existing trained models without modification |
| **Optional adapter** | Can use Phase 4 adapter or internal observation building |
| **Action masking support** | Handle both MaskablePPO and standard algorithms |
| **Fallback on infeasible** | Robustness when model predicts invalid action |
| **Deterministic prediction** | Consistent evaluation behavior |
| **from_file classmethod** | Convenient loading from saved models |
| **set_adapter method** | Deferred adapter injection for dependency management |
| **update() no-op** | Pre-trained models don't learn online |

### 3. Integration with Phase 4

```python
# Phase 4: Direct SB3 usage (unchanged)
env = UnifiedSimEnv(config)
model = PPO("MlpPolicy", env)
model.learn(total_timesteps=10000)
model.save("trained_ppo.zip")

# Phase 5: Use model through ControlPolicy interface
rl_policy = RLPolicy.from_file("trained_ppo.zip", algorithm="PPO")

# Both patterns work together
orchestrator = SDNOrchestrator(config, pipelines, policy=rl_policy)
```

### 4. Thread Safety Note

```python
class ThreadSafeRLPolicy(RLPolicy):
    """Thread-safe wrapper for parallel simulations."""

    def __init__(self, model: BaseAlgorithm, **kwargs: Any) -> None:
        super().__init__(model, **kwargs)
        import threading
        self._lock = threading.Lock()

    def select_action(
        self,
        request: Request,
        options: list[PathOption],
        network_state: NetworkState,
    ) -> int:
        with self._lock:
            return super().select_action(request, options, network_state)
```

---

## Verification

- [ ] RLPolicy wraps SB3 BaseAlgorithm models
- [ ] select_action builds observation correctly
- [ ] Action masking enforced for feasibility
- [ ] Works with both MaskablePPO and standard algorithms
- [ ] Fallback to first feasible on invalid prediction
- [ ] get_name() returns descriptive string
- [ ] from_file() loads saved models
- [ ] update() is no-op
- [ ] Comprehensive logging included

---

## Test Cases to Implement

```python
import pytest
from unittest.mock import MagicMock, patch

class TestRLPolicy:
    """Tests for RLPolicy wrapper."""

    @pytest.fixture
    def mock_model(self):
        model = MagicMock()
        model.predict.return_value = (1, None)
        return model

    @pytest.fixture
    def sample_options(self):
        return [
            PathOption(path_index=0, is_feasible=False, ...),
            PathOption(path_index=1, is_feasible=True, ...),
            PathOption(path_index=2, is_feasible=True, ...),
        ]

    def test_select_action_uses_model(self, mock_model, sample_options):
        """Should call model.predict with observation."""
        policy = RLPolicy(mock_model)
        action = policy.select_action(mock_request, sample_options, mock_state)
        assert mock_model.predict.called
        assert action == 1

    def test_infeasible_prediction_falls_back(self, mock_model, sample_options):
        """Should select first feasible when model returns infeasible."""
        mock_model.predict.return_value = (0, None)  # Index 0 is infeasible
        policy = RLPolicy(mock_model)
        action = policy.select_action(mock_request, sample_options, mock_state)
        assert action == 1  # First feasible

    def test_all_infeasible_returns_negative_one(self, mock_model):
        """Should return -1 when no feasible options."""
        infeasible_options = [
            PathOption(path_index=i, is_feasible=False, ...)
            for i in range(3)
        ]
        policy = RLPolicy(mock_model)
        action = policy.select_action(mock_request, infeasible_options, mock_state)
        assert action == -1

    def test_get_name_includes_model_type(self, mock_model):
        """Should include model class name."""
        policy = RLPolicy(mock_model)
        assert "Mock" in policy.get_name()  # MagicMock

    def test_update_is_noop(self, mock_model):
        """update() should not raise or modify state."""
        policy = RLPolicy(mock_model)
        policy.update(mock_request, 1, 1.0)  # Should not raise

    def test_from_file_loads_model(self):
        """Should load model from file."""
        with patch("stable_baselines3.PPO.load") as mock_load:
            mock_load.return_value = MagicMock()
            policy = RLPolicy.from_file("model.zip", algorithm="PPO")
            mock_load.assert_called_with("model.zip")
```

---

## Next Task

Proceed to **P5.1.f** to implement RLPolicy with full test coverage.
