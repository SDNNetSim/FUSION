[general_settings]
sim_type = yue
holding_time = 0.2
arrival_rate = {'start': 150, 'stop': 152, 'step': 50}
thread_erlangs = False
guard_slots = 1
num_requests = 25000
request_distribution = {"25": 0.3, "50": 0.5, "100": 0.2, "200": 0.0, "400": 0.0}
max_iters = 50
max_segments = 1
dynamic_lps = False
allocation_method = first_fit
k_paths=5
route_method = k_shortest_path
save_snapshots = False
snapshot_step = 10
print_step = 1

[topology_settings]
network = NSFNet
spectral_slots = 128
bw_per_slot = 12.5
cores_per_link = 7
const_link_weight = False

[snr_settings]
snr_type = None
xt_type = without_length
beta = 0.5
theta = 0.0
input_power = 0.001
egn_model = False
phi = {"QPSK": 1, "16-QAM": 0.68, "64-QAM": 0.6190476190476191}
bi_directional = True
xt_noise = False
requested_xt = {"QPSK": -26.19, "16-QAM": -36.69, "64-QAM": -41.69}

# TODO: Repeat ml/rl setting to go to a general AI settings
# TODO: Update example files (add one for ML and RL)
[rl_settings]
# For RL Zoo
device = cpu
optimize = False
is_training = True
path_algorithm = armed_bandit
path_model = ql/NSFNet/0512/12_51_37_625844
core_algorithm = first_fit
core_model = ql/NSFNet/0512/12_55_01_531372
spectrum_algorithm = first_fit
spectrum_model = ppo/NSFNet/0512/12_57_55_484293
# Only for DRL
render_mode = None
super_channel_space = 3
# Only for q-learning
learn_rate = 0.2
discount_factor = 0.01
epsilon_start = 0.05
epsilon_end = 0.05
reward = 1
penalty = -100

[ml_settings]
deploy_model = False
output_train_data = False
ml_training = True
ml_model = decision_tree
train_file_path = Pan-European/0531/22_00_16_630834
# train_file_path = USNet/0531/21_16_21_157019
test_size = 0.3

[file_settings]
file_type = json