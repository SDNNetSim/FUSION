[general_settings]
sim_type = arash
holding_time = 3600.0
erlangs = {'start': 50, 'stop': 650, 'step': 50}
thread_erlangs = True
guard_slots = 1
num_requests = 25000
request_distribution = {"25": 0.0, "50": 0.1, "100": 0.5, "200": 0.3, "400": 0.1}
max_iters = 100
max_segments = 8
dynamic_lps = False
allocation_method = priority_first
k_paths=3
route_method = k_shortest_path
save_snapshots = False
snapshot_step = 10
print_step = 1

[topology_settings]
network = USNet
spectral_slots = 320
bw_per_slot = 12.5
cores_per_link = 7
const_link_weight = False

[snr_settings]
snr_type = xt_calculation
xt_type = without_length
beta = 0.5
theta = 0.0
input_power = 0.001
egn_model = False
phi = {"QPSK": 1, "16-QAM": 0.68, "64-QAM": 0.6190476190476191}
bi_directional = True
xt_noise = False
requested_xt = {"QPSK": -26.19, "16-QAM": -36.69, "64-QAM": -41.69}

# TODO: Repeat ml/rl setting to go to a general AI settings
# TODO: Update example files (add one for ML and RL)
[rl_settings]
# For RL Zoo
device = cpu
optimize = False
is_training = True
path_algorithm = first_fit
path_model = ql/NSFNet/0512/12_51_37_625844
core_algorithm = q_learning
core_model = ql/NSFNet/0512/12_55_01_531372
spectrum_algorithm = first_fit
spectrum_model = ppo/NSFNet/0512/12_57_55_484293
# Only for DRL
render_mode = None
super_channel_space = 3
# Only for q-learning
learn_rate = 0.01
discount_factor = 0.9
epsilon_start = 0.3
epsilon_end = 0.05
reward = 1
penalty = -1

[ml_settings]
deploy_model = True
output_train_data = False
ml_training = True
ml_model = decision_tree
train_file_path = Pan-European/0531/22_00_16_630834
# train_file_path = USNet/0531/21_16_21_157019
test_size = 0.3

[file_settings]
file_type = json
