# =============================================================================
# FUSION Configuration Template - Policy and Protection Example (P5.6)
# =============================================================================
# This template demonstrates the P5.6 policy and protection configuration.
# Features:
# - Load-balanced heuristic policy selection
# - 1+1 link-disjoint protection enabled
# - Compatible with standard simulation settings

[general_settings]
# Core simulation parameters
holding_time = 0.2
mod_assumption = DEFAULT
mod_assumption_path = data/json_input/example_mods/default_mod_formats.json

# Traffic load parameters
erlang_start = 300
erlang_stop = 900
erlang_step = 300

# Simulation control
max_iters = 3
guard_slots = 1
max_segments = 4
thread_erlangs = False
dynamic_lps = False
fixed_grid = False
pre_calc_mod_selection = False
spectrum_priority = None

# Request generation
num_requests = 200
request_distribution = {"25": 0.20, "50": 0.20, "100": 0.40, "200": 0.20}

# Output and monitoring
save_snapshots = False
snapshot_step = 10
print_step = 50
save_step = 10
save_start_end_slots = False

# Feature toggles
is_grooming_enabled = False
can_partially_serve = False
transponder_usage_per_node = False
blocking_type_ci = False

[topology_settings]
# Network topology configuration
network = NSFNet
bw_per_slot = 12.5
cores_per_link = 1
const_link_weight = False
is_only_core_node = True
multi_fiber = False

[routing_settings]
# Routing configuration
route_method = k_shortest_path
k_paths = 3

[spectrum_settings]
# Spectrum management
c_band = 320
guard_slots = 1
allocation_method = first_fit

# =============================================================================
# P5.6: Policy Configuration
# =============================================================================
[policy_settings]
# Policy type: heuristic, ml, or rl
policy_type = heuristic

# Heuristic policy name (when policy_type = heuristic):
# - first_feasible: Select first feasible path (K-shortest first fit)
# - shortest: Select shortest feasible path by distance
# - least_congested: Select least congested feasible path
# - random: Randomly select among feasible paths
# - load_balanced: Balance path length and congestion
policy_name = load_balanced

# For ML/RL policies (when policy_type = ml or rl):
# model_path = /path/to/trained_model.zip
# fallback_policy = first_feasible
# algorithm = PPO
# device = cpu

# Number of candidate paths for policy selection
k_paths = 3

# =============================================================================
# P5.6: Heuristic Policy Parameters
# =============================================================================
[heuristic_settings]
# Alpha parameter for LoadBalancedPolicy (0.0 to 1.0)
# - 0.0 = prioritize low congestion (like LeastCongestedPolicy)
# - 0.5 = equal balance between length and congestion
# - 1.0 = prioritize short length (like ShortestFeasiblePolicy)
alpha = 0.5

# Random seed for RandomFeasiblePolicy (comment out for non-reproducible)
# seed = 42

# =============================================================================
# P5.6: Protection Configuration
# =============================================================================
[protection_settings]
# Enable 1+1 dedicated protection
protection_enabled = True

# Disjointness type: link or node
# - link: Paths share no common edges (may share intermediate nodes)
# - node: Paths share no common intermediate nodes (stronger guarantee)
disjointness_type = link

# Protection switchover latency (milliseconds)
protection_switchover_ms = 50.0

# Restoration latency after failure (milliseconds)
restoration_latency_ms = 100.0

# Revert to primary after repair
revert_to_primary = True

# =============================================================================
# Standard Settings (unchanged from default.ini)
# =============================================================================
[snr_settings]
# Signal-to-noise ratio configuration
snr_type = None
xt_type = without_length
beta = 0.5
theta = 0.0
input_power = 0.001
egn_model = False
phi = {"QPSK": 1, "16-QAM": 0.68, "64-QAM": 0.6190476190476191}
bi_directional = True
xt_noise = False
requested_xt = {"QPSK": -26.19, "16-QAM": -36.69, "64-QAM": -41.69}
snr_recheck = False
recheck_adjacent_cores = False
recheck_crossband = False

[rl_settings]
# Reinforcement learning configuration (not used when policy_type = heuristic)
obs_space = obs_3
n_trials = 1
device = cpu
optimize_hyperparameters = False
optuna_trials = 1
is_training = False
path_algorithm = first_fit
path_model = None
core_algorithm = first_fit
core_model = None
spectrum_algorithm = first_fit
spectrum_model = None
render_mode = None
super_channel_space = 3
alpha_start = 0.000215
alpha_end = 0.000215
alpha_update = linear_decay
gamma = 0.1
epsilon_start = 0.01
epsilon_end = 0.01
epsilon_update = exp_decay
path_levels = 2
decay_rate = 0.4
feature_extractor = path_gnn
gnn_type = graph_conv
layers = 2
emb_dim = 64
heads = 4
conf_param = 2
cong_cutoff = 0.9
reward = 1
penalty = -10
dynamic_reward = False
core_beta = 0.1

[ml_settings]
# Machine learning configuration
deploy_model = False
output_train_data = False
ml_training = False
ml_model = None
train_file_path = None
test_size = 0.3

[file_settings]
# File format settings
file_type = json
